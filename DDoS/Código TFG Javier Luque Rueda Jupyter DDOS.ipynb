{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javier\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importación de librerías necesarias para el procesamiento de datos y análisis\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import sklearn\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc,accuracy_score, precision_score, recall_score, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARGA DE MODELOS\n",
    "\n",
    "def cargar_datos():\n",
    "    # Verificar si los archivos con los datos ya existen\n",
    "    \n",
    "    if os.path.exists('datos_X_DDOS.csv') and os.path.exists('datos_Y_DDOS.csv'):\n",
    "        X = pd.read_csv('datos_X_DDOS.csv')\n",
    "       # y = pd.read_csv('datos_y.csv', header=None, squeeze=True)  # Usar squeeze=True para obtener una Serie\n",
    "        y = pd.read_csv('datos_Y_DDOS.csv', header=None, skiprows=1).iloc[:, 0]  # Obtener la Serie de una sola columna\n",
    "    else:\n",
    "        # Recopilación de archivos CSV de un directorio y concatenación en un DataFrame\n",
    "        csv_files = []\n",
    "        for dirname, _, filenames in os.walk('dataset\\MachineLearningCSV_reducido\\MachineLearningCVE'):\n",
    "            for filename in filenames:\n",
    "                csv_file = os.path.join(dirname, filename)\n",
    "                print(os.path.join(dirname, filename))\n",
    "                csv_files.append(csv_file)\n",
    "        df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "        # Limpieza de datos: eliminación de espacios en nombres de columnas y valores nulos o infinitos\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Preprocesamiento de la columna 'Label' para agrupar categorías de ataques similares\n",
    "        df_experiment = df.copy(deep=True)\n",
    "        df_experiment.Label.replace(\"Web.*\", \"Web Attack\", regex=True, inplace=True)\n",
    "        df_experiment.Label.replace(r'.*Patator$', \"Brute Force\", regex=True,inplace=True)\n",
    "        df_experiment.Label.replace([\"DoS GoldenEye\", \"DoS Hulk\", \"DoS Slowhttptest\", \"DoS slowloris\"], \"DDoS\", inplace=True)\n",
    "\n",
    "        # División de los datos en conjuntos de entrenamiento y prueba\n",
    "        y = df_experiment.Label\n",
    "        X = df_experiment.drop(columns='Label')\n",
    "\n",
    "        # Guardar los datos en archivos CSV\n",
    "        X.to_csv('datos_X_DDOS.csv', index=False)\n",
    "        y.to_csv('datos_Y_DDOS.csv', index=False)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Definir una función para realizar validación cruzada y obtener métricas de rendimiento\n",
    "def cross_validate_model(model, X, y,model_name,cv=5):\n",
    "    try: \n",
    "        scores = joblib.load(f'{model_name}_cross_val_results.pkl')      \n",
    "    except FileNotFoundError:\n",
    "        scores = cross_val_score(model, X, y, cv=cv)\n",
    "        joblib.dump(scores,f'{model_name}_cross_val_results.pkl')\n",
    "\n",
    "    print(\"Puntuaciones de validación cruzada:\", scores)\n",
    "    print(\"Media de puntuaciones:\", scores.mean())\n",
    "    print(\"Desviación estándar de puntuaciones:\", scores.std())\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, X_selected, y, model_name):\n",
    "    # Realizar predicciones y calcular la puntuación del modelo\n",
    "    try: \n",
    "        score = joblib.load(f'{model_name}_score.pkl')\n",
    "    except FileNotFoundError:\n",
    "        score = model.score(X_test, y_test)\n",
    "        joblib.dump(score,f'{model_name}_score.pkl')\n",
    "\n",
    "    print(f\"Puntuación del {model_name}: {score}\") \n",
    "    print(f\"Validación cruzada para {model_name}:\")\n",
    "    cross_validate_model(model, X_selected, y, model_name)\n",
    "    mostrar_ejemplos_predicciones(model, model_name, X_test, y_test)\n",
    "   \n",
    "def test_svm_models(X_train, X_test, y_train, y_test, kernel, degree=0):\n",
    "\n",
    "    if kernel == 'poly':\n",
    "        svm_model = SVC(kernel=kernel, degree=degree)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "        joblib.dump(svm_model, f'svm_model_{kernel}_degree_{degree}DDOS.pkl')\n",
    "        score = svm_model.score(X_test, y_test)\n",
    "        print(f\"Score del SVM con kernel {kernel}: {score:.4f}: \")\n",
    "    else:\n",
    "        svm_model = SVC(kernel=kernel)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "        joblib.dump(svm_model, f'svm_model_{kernel}DDOS.pkl')\n",
    "        score = svm_model.score(X_test, y_test)\n",
    "        print(f\"Score del SVM con kernel {kernel}: {score:.4f}: \")\n",
    "\n",
    "# Función para mostrar ejemplos de predicciones\n",
    "def mostrar_ejemplos_predicciones(modelo, modelo_name,X_test, y_test, num_ejemplos=5):\n",
    "    predicciones = modelo.predict(X_test)\n",
    "    print(\"\\nMatriz de confusión del modelo: \"+f'{modelo_name}')\n",
    "    print(confusion_matrix(y_test, predicciones))\n",
    "    print(\"\\nInforme de clasificación del modelo: \"+f'{modelo_name}')\n",
    "    print(classification_report(y_test, predicciones))\n",
    "\n",
    "    # print(\"Ejemplos de predicciones:\")\n",
    "    # for i in range(num_ejemplos):\n",
    "    #     ejemplo_prediccion = f\"Ejemplo {i+1}: Predicción={predicciones[i]}, Verdadero={y_test.iloc[i]}\"\n",
    "    #     print(ejemplo_prediccion)\n",
    "\n",
    "# Función para graficar la curva ROC de todos los modelos\n",
    "def plot_roc_curves(models, X_test, y_test, y_test_encoded):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name, model in models.items():\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_score = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_score = model.decision_function(X_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test_encoded, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javier\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Javier\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Javier\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## CARGA DE DATOS\n",
    "\n",
    "# Si tienes problemas  de rendimiento en algún punto puedes cargar los modelos, valores SHAP, explicadores, calibradores etc \n",
    "# desde mi github (https://github.com/javierluquerueda/XAI-para-la-Deteccion-Confiable-de-Intrusiones-Explicabilidad-e-Interpretabilidad).\n",
    "\n",
    "X,y = cargar_datos()\n",
    "\n",
    "try: \n",
    "    X_scaled = joblib.load('X_scaled.pkl')\n",
    "    top_20_feature_names = joblib.load('top_20_feature_names.pkl')\n",
    "except FileNotFoundError:\n",
    "    forest = RandomForestClassifier()\n",
    "    forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    joblib.dump(indices,'indices.pkl')\n",
    "    top_20_feature_indices = indices[:20]  # Índices de las 20 características más importantes\n",
    "    top_20_feature_names = X.columns[top_20_feature_indices].tolist() # Nombres de las 20 características más importantes\n",
    "    joblib.dump(top_20_feature_names,'top_20_feature_names.pkl')\n",
    "    X_selected = X.values[:, indices[:20]]  # Seleccionar las 20 características más importantes\n",
    "    # Normalizar los datos\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    joblib.dump(X_scaled,'X_scaled.pkl')\n",
    "\n",
    "try: \n",
    "    X_train = joblib.load('X_train.pkl')\n",
    "    X_test = joblib.load('X_test.pkl')\n",
    "    y_train = joblib.load('y_train.pkl')\n",
    "    y_test = joblib.load('y_test.pkl')\n",
    "    X_train_df = pd.DataFrame(X_train, columns=top_20_feature_names)\n",
    "    X_test_df = pd.DataFrame(X_test, columns=top_20_feature_names)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    joblib.dump(X_train,'X_train.pkl')\n",
    "    joblib.dump(X_test,'X_test.pkl')\n",
    "    joblib.dump(y_train,'y_train.pkl')\n",
    "    joblib.dump(y_test,'y_test.pkl')\n",
    "\n",
    "try:\n",
    "    randomForest_modelDDOS = joblib.load('randomForest_model_DDOS.pkl')\n",
    "except FileNotFoundError:\n",
    "    randomForest_modelDDOS = RandomForestClassifier()\n",
    "    randomForest_modelDDOS.fit(X_train, y_train)\n",
    "    joblib.dump(randomForest_modelDDOS, 'randomForest_model_DDOS.pkl')\n",
    "\n",
    "try:\n",
    "    svm_model_linearDDOS = joblib.load('svm_model_linearDDOS.pkl')\n",
    "except FileNotFoundError:\n",
    "    test_svm_models(X_train, X_test, y_train, y_test, \"linear\")\n",
    "\n",
    "try:\n",
    "    svm_model_poly_degree_2DDOS = joblib.load('svm_model_poly_degree_2DDOS.pkl')\n",
    "except FileNotFoundError:\n",
    "    test_svm_models(X_train, X_test, y_train, y_test, \"poly\", 2)\n",
    "\n",
    "try:\n",
    "    svm_model_poly_degree_3DDOS = joblib.load('svm_model_poly_degree_3DDOS.pkl')\n",
    "except FileNotFoundError:\n",
    "    test_svm_models(X_train, X_test, y_train, y_test, \"poly\", 3)\n",
    "\n",
    "try:\n",
    "    svm_model_rbfDDOS = joblib.load('svm_model_rbfDDOS.pkl')\n",
    "except FileNotFoundError:\n",
    "    test_svm_models(X_train, X_test, y_train, y_test, \"rbf\")\n",
    "\n",
    "try: \n",
    "    lgb_model_DDOS = joblib.load('lgb_model_DDOS.pkl')\n",
    "    #evaluate_model(lgb_model_DDOS, X_test, y_test, X_scaled, y, \"lgbModel\")\n",
    "except FileNotFoundError:\n",
    "    lgb_model = lgb.LGBMClassifier(verbosity=-1)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    joblib.dump(lgb_model, 'lgb_model_DDOS.pkl')\n",
    "\n",
    "try: \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    y_encoded = label_encoder.transform(y)\n",
    "    y_test_encoded_series = pd.Series(y_test_encoded)\n",
    "    xgBoost_model_DDOS = joblib.load('xgBoost_model_DDOS.pkl')\n",
    "    #evaluate_model(xgBoost_model_DDOS, X_test, y_test_encoded_series, X_scaled, y_encoded, \"xgBoost\")\n",
    "except FileNotFoundError:\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    xgBoost_model = xgb.XGBClassifier()\n",
    "    xgBoost_model.fit(X_train, y_train_encoded)\n",
    "    joblib.dump(xgBoost_model, 'xgBoost_model_DDOS.pkl')\n",
    "\n",
    "try: \n",
    "    logisticRegression_model_DDOS = joblib.load('logisticRegression_model_DDOS.pkl')\n",
    "    #evaluate_model(logisticRegression_model_DDOS, X_test, y_test, X_scaled, y, \"logistic\")\n",
    "except FileNotFoundError:\n",
    "    # Entrenamiento del modelo\n",
    "    logistic_model = LogisticRegression(penalty='l2', solver='saga', max_iter=10000)\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    joblib.dump(logistic_model, 'logisticRegression_model_DDOS.pkl')\n",
    "\n",
    "try: \n",
    "    mlp_model_DDOS = joblib.load('mlp_model_DDOS.pkl')\n",
    "    #evaluate_model(mlp_model_DDOS,X_test,y_test,X_scaled,y,\"mlp_model\")\n",
    "except FileNotFoundError:\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, solver='adam', random_state=42)\n",
    "    mlp_model.fit(X_train,y_train)\n",
    "    joblib.dump(mlp_model,'mlp_model_DDOS.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de modelos calibrados\n",
    "# Importante para los SVM hay que calibrarlos con CalibratedClassifierCV\n",
    "# para habilitar la predicción de probabilidades (predict_proba), Aunque establecer probability=True permite \n",
    "# la estimación de probabilidades, puedes mejorar la calidad de estas probabilidades calibrando el modelo con \n",
    "# CalibratedClassifierCV.\n",
    "\n",
    "try:\n",
    "    calibrated_svmLineal = joblib.load('calibrated_svmLineal')\n",
    "except FileNotFoundError:\n",
    "    calibrated_svmLineal = CalibratedClassifierCV(svm_model_linearDDOS, method='sigmoid',cv='prefit')\n",
    "    calibrated_svmLineal.fit(X_train, y_train)\n",
    "    joblib.dump(calibrated_svmLineal, 'calibrated_svmLineal')\n",
    "\n",
    "try:\n",
    "    calibrated_svmPoly2 = joblib.load('calibrated_svmPoly2')\n",
    "except FileNotFoundError:\n",
    "    calibrated_svmPoly2 = CalibratedClassifierCV(svm_model_poly_degree_2DDOS, method='sigmoid',cv='prefit')\n",
    "    calibrated_svmPoly2.fit(X_train, y_train)\n",
    "    joblib.dump(calibrated_svmPoly2, 'calibrated_svmPoly2')\n",
    "\n",
    "try:\n",
    "    calibrated_svmPoly3 = joblib.load('calibrated_svmPoly3')\n",
    "except FileNotFoundError:\n",
    "    calibrated_svmPoly3 = CalibratedClassifierCV(svm_model_poly_degree_3DDOS, method='sigmoid',cv='prefit')\n",
    "    calibrated_svmPoly3.fit(X_train, y_train)\n",
    "    joblib.dump(calibrated_svmPoly3, 'calibrated_svmPoly3')\n",
    "\n",
    "try:\n",
    "    calibrated_svmRbf = joblib.load('calibrated_svmRbf')\n",
    "except FileNotFoundError:\n",
    "    calibrated_svmRbf = CalibratedClassifierCV(svm_model_rbfDDOS, method='sigmoid',cv='prefit')\n",
    "    calibrated_svmRbf.fit(X_train, y_train)\n",
    "    joblib.dump(calibrated_svmRbf, 'calibrated_svmRbf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuación del randomForest_model_DDOS: 0.9998006335422989\n",
      "Validación cruzada para randomForest_model_DDOS:\n",
      "Puntuaciones de validación cruzada: [0.99869304 0.99988924 0.99977848 0.99968987 0.99743033]\n",
      "Media de puntuaciones: 0.9990961911610381\n",
      "Desviación estándar de puntuaciones: 0.0009364303114702419\n",
      "\n",
      "Matriz de confusión del modelo: randomForest_model_DDOS\n",
      "[[19537     1]\n",
      " [    8 25597]]\n",
      "\n",
      "Informe de clasificación del modelo: randomForest_model_DDOS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     19538\n",
      "        DDoS       1.00      1.00      1.00     25605\n",
      "\n",
      "    accuracy                           1.00     45143\n",
      "   macro avg       1.00      1.00      1.00     45143\n",
      "weighted avg       1.00      1.00      1.00     45143\n",
      "\n",
      "\n",
      "\n",
      "Puntuación del logistic: 0.9856677668741555\n",
      "Validación cruzada para logistic:\n",
      "Puntuaciones de validación cruzada: [0.95199699 0.97760401 0.98591112 0.97069248 0.9522396 ]\n",
      "Media de puntuaciones: 0.96768883967486\n",
      "Desviación estándar de puntuaciones: 0.013596303179981464\n",
      "\n",
      "Matriz de confusión del modelo: logistic\n",
      "[[18932   606]\n",
      " [   41 25564]]\n",
      "\n",
      "Informe de clasificación del modelo: logistic\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      0.97      0.98     19538\n",
      "        DDoS       0.98      1.00      0.99     25605\n",
      "\n",
      "    accuracy                           0.99     45143\n",
      "   macro avg       0.99      0.98      0.99     45143\n",
      "weighted avg       0.99      0.99      0.99     45143\n",
      "\n",
      "\n",
      "\n",
      "Puntuación del mlp_model: 0.9994240524555302\n",
      "Validación cruzada para mlp_model:\n",
      "Puntuaciones de validación cruzada: [0.9988481  0.99937974 0.99942404 0.99960126 0.99864871]\n",
      "Media de puntuaciones: 0.9991803692875596\n",
      "Desviación estándar de puntuaciones: 0.00036587765359337367\n",
      "\n",
      "Matriz de confusión del modelo: mlp_model\n",
      "[[19532     6]\n",
      " [   20 25585]]\n",
      "\n",
      "Informe de clasificación del modelo: mlp_model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     19538\n",
      "        DDoS       1.00      1.00      1.00     25605\n",
      "\n",
      "    accuracy                           1.00     45143\n",
      "   macro avg       1.00      1.00      1.00     45143\n",
      "weighted avg       1.00      1.00      1.00     45143\n",
      "\n",
      "\n",
      "\n",
      "Puntuación del lgbModel: 0.9998006335422989\n",
      "Validación cruzada para lgbModel:\n",
      "Puntuaciones de validación cruzada: [0.99984494 0.99997785 0.99988924 0.99988924 0.99931328]\n",
      "Media de puntuaciones: 0.9997829079573913\n",
      "Desviación estándar de puntuaciones: 0.000238752446176724\n",
      "\n",
      "Matriz de confusión del modelo: lgbModel\n",
      "[[19537     1]\n",
      " [    8 25597]]\n",
      "\n",
      "Informe de clasificación del modelo: lgbModel\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     19538\n",
      "        DDoS       1.00      1.00      1.00     25605\n",
      "\n",
      "    accuracy                           1.00     45143\n",
      "   macro avg       1.00      1.00      1.00     45143\n",
      "weighted avg       1.00      1.00      1.00     45143\n",
      "\n",
      "\n",
      "\n",
      "Puntuación del xgBoost: 0.9998006335422989\n",
      "Validación cruzada para xgBoost:\n",
      "Puntuaciones de validación cruzada: [0.99982279 0.99997785 0.99991139 0.99997785 0.99933543]\n",
      "Media de puntuaciones: 0.999805060374882\n",
      "Desviación estándar de puntuaciones: 0.0002416127146217042\n",
      "\n",
      "Matriz de confusión del modelo: xgBoost\n",
      "[[19537     1]\n",
      " [    8 25597]]\n",
      "\n",
      "Informe de clasificación del modelo: xgBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19538\n",
      "           1       1.00      1.00      1.00     25605\n",
      "\n",
      "    accuracy                           1.00     45143\n",
      "   macro avg       1.00      1.00      1.00     45143\n",
      "weighted avg       1.00      1.00      1.00     45143\n",
      "\n",
      "\n",
      "\n",
      "Puntuación del svm_model_linearDDOS: 0.9985158274815586\n",
      "Validación cruzada para svm_model_linearDDOS:\n",
      "Puntuaciones de validación cruzada: [0.99793988 0.99922467 0.99891454 0.99862656 0.99760755]\n",
      "Media de puntuaciones: 0.9984626381645476\n",
      "Desviación estándar de puntuaciones: 0.0006026980320522039\n",
      "\n",
      "Matriz de confusión del modelo: svm_model_linearDDOS\n",
      "[[19507    31]\n",
      " [   36 25569]]\n",
      "\n",
      "Informe de clasificación del modelo: svm_model_linearDDOS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     19538\n",
      "        DDoS       1.00      1.00      1.00     25605\n",
      "\n",
      "    accuracy                           1.00     45143\n",
      "   macro avg       1.00      1.00      1.00     45143\n",
      "weighted avg       1.00      1.00      1.00     45143\n",
      "\n",
      "\n",
      "\n",
      "Puntuación del svm_model_poly_degree_2DDOS: 0.9953924196442416\n",
      "Validación cruzada para svm_model_poly_degree_2DDOS:\n",
      "Puntuaciones de validación cruzada: [0.99313293 0.99743033 0.99791768 0.99590182 0.99240175]\n",
      "Media de puntuaciones: 0.9953569042889854\n",
      "Desviación estándar de puntuaciones: 0.0022285678316463038\n",
      "\n",
      "Matriz de confusión del modelo: svm_model_poly_degree_2DDOS\n",
      "[[19360   178]\n",
      " [   30 25575]]\n",
      "\n",
      "Informe de clasificación del modelo: svm_model_poly_degree_2DDOS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      0.99      0.99     19538\n",
      "        DDoS       0.99      1.00      1.00     25605\n",
      "\n",
      "    accuracy                           1.00     45143\n",
      "   macro avg       1.00      0.99      1.00     45143\n",
      "weighted avg       1.00      1.00      1.00     45143\n",
      "\n",
      "\n",
      "\n",
      "Puntuación del svm_model_poly_degree_3DDOS: 0.9957246970737434\n",
      "Validación cruzada para svm_model_poly_degree_3DDOS:\n",
      "Puntuaciones de validación cruzada: [0.97629754 0.99798414 0.99771831 0.99645563 0.99315493]\n",
      "Media de puntuaciones: 0.9923221111269178\n",
      "Desviación estándar de puntuaciones: 0.008194435517418516\n",
      "\n",
      "Matriz de confusión del modelo: svm_model_poly_degree_3DDOS\n",
      "[[19366   172]\n",
      " [   21 25584]]\n",
      "\n",
      "Informe de clasificación del modelo: svm_model_poly_degree_3DDOS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      0.99      1.00     19538\n",
      "        DDoS       0.99      1.00      1.00     25605\n",
      "\n",
      "    accuracy                           1.00     45143\n",
      "   macro avg       1.00      1.00      1.00     45143\n",
      "weighted avg       1.00      1.00      1.00     45143\n",
      "\n",
      "\n",
      "\n",
      "Puntuación del svm_model_rbfDDOS: 0.9982057018806902\n",
      "Validación cruzada para svm_model_rbfDDOS:\n",
      "Puntuaciones de validación cruzada: [0.99718672 0.99880377 0.99904745 0.99820566 0.99658854]\n",
      "Media de puntuaciones: 0.997966429548014\n",
      "Desviación estándar de puntuaciones: 0.0009416482701801424\n",
      "\n",
      "Matriz de confusión del modelo: svm_model_rbfDDOS\n",
      "[[19485    53]\n",
      " [   28 25577]]\n",
      "\n",
      "Informe de clasificación del modelo: svm_model_rbfDDOS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     19538\n",
      "        DDoS       1.00      1.00      1.00     25605\n",
      "\n",
      "    accuracy                           1.00     45143\n",
      "   macro avg       1.00      1.00      1.00     45143\n",
      "weighted avg       1.00      1.00      1.00     45143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## EVALUACIONES DE ACCURACY Y VALIDACION CRUZADA\n",
    "# print(top_20_feature_names)\n",
    "# print(\"Etiquetas únicas y cantidad de clases:\")\n",
    "# labels = y.unique()\n",
    "# classes = y.nunique()\n",
    "# print(\"labels:\", labels) \n",
    "# print(\"classes\", classes)\n",
    "\n",
    "# Evaluar cada modelo, precisión, puntuaciones de validación cruzada, su media y desviación estándar \n",
    "\n",
    "evaluate_model(randomForest_modelDDOS, X_test, y_test, X_scaled, y, \"randomForest_model_DDOS\")\n",
    "print(\"\\n\")\n",
    "evaluate_model(logisticRegression_model_DDOS, X_test, y_test, X_scaled, y, \"logistic\")\n",
    "print(\"\\n\")\n",
    "evaluate_model(mlp_model_DDOS, X_test, y_test, X_scaled, y, \"mlp_model\")\n",
    "print(\"\\n\")\n",
    "evaluate_model(lgb_model_DDOS, X_test, y_test, X_scaled, y, \"lgbModel\")\n",
    "print(\"\\n\")\n",
    "evaluate_model(xgBoost_model_DDOS, X_test, y_test_encoded_series, X_scaled, y_encoded, \"xgBoost\")\n",
    "print(\"\\n\")\n",
    "evaluate_model(svm_model_linearDDOS, X_test, y_test, X_scaled, y, \"svm_model_linearDDOS\")\n",
    "print(\"\\n\")\n",
    "evaluate_model(svm_model_poly_degree_2DDOS, X_test, y_test, X_scaled, y, \"svm_model_poly_degree_2DDOS\")\n",
    "print(\"\\n\")\n",
    "evaluate_model(svm_model_poly_degree_3DDOS, X_test, y_test, X_scaled, y, \"svm_model_poly_degree_3DDOS\")\n",
    "print(\"\\n\")\n",
    "evaluate_model(svm_model_rbfDDOS, X_test, y_test, X_scaled, y, \"svm_model_rbfDDOS\") \n",
    "\n",
    "\n",
    "models = {\n",
    "    'Random Forest': randomForest_modelDDOS,\n",
    "    'Logistic Regression': logisticRegression_model_DDOS,\n",
    "    'MLP': mlp_model_DDOS,\n",
    "    'LightGBM': lgb_model_DDOS,\n",
    "    'XGBoost': xgBoost_model_DDOS,\n",
    "    'SVM Linear': svm_model_linearDDOS,\n",
    "    'SVM Poly Degree 2': svm_model_poly_degree_2DDOS,\n",
    "    'SVM Poly Degree 3': svm_model_poly_degree_3DDOS,\n",
    "    'SVM RBF': svm_model_rbfDDOS\n",
    "}\n",
    "\n",
    "# Descomentar para mostrar la curva ROC de todos los modelos, tarda 10 min aprox\n",
    "# plot_roc_curves(models, X_test, y_test, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPLICABILIDAD GLOBAL\n",
    "#IMPORTANCIA DE CARACTERÍSTICAS CON COEFICIENTES DEL MODELO\n",
    "\n",
    "# Obtener los coeficientes del modelo SVM lineal o Logistic Regression, solo funciona con los modelos que se \n",
    "# pueden ajustar linealmente, para ello llamar a la función (en la última celda) con el modelo y los nombres \n",
    "# de las 20 características. Ejemplos de como se llamaría con cada uno de los modelos:\n",
    "\n",
    "# explicabilidad_global(logisticRegression_model_DDOS, top_20_feature_names)\n",
    "# explicabilidad_global(svm_model_linearDDOS, top_20_feature_names)\n",
    "\n",
    "# Function to display global feature importance\n",
    "def explicabilidad_global(model, feature_names):\n",
    "    try:\n",
    "        coeficientes = model.coef_[0]\n",
    "        importancias = np.abs(coeficientes)\n",
    "    except AttributeError:\n",
    "        print(\"Este modelo no proporciona coeficientes directamente.\")\n",
    "        return\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    importancias_normalizadas = scaler.fit_transform(importancias.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    plt.bar(range(len(importancias_normalizadas)), importancias_normalizadas)\n",
    "    plt.xticks(range(len(importancias_normalizadas)), feature_names, rotation='vertical')\n",
    "    plt.xlabel('Características')\n",
    "    plt.ylabel('Importancia')\n",
    "    plt.title('Importancia de características del modelo')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VALORES DE SHAP PARA CADA CARACTERÍSTICA DE MANERA GLOBAL\n",
    "\n",
    "# Calcula los valores SHAP para todas las características e instancias, tiene problemas de rendimiento con el\n",
    "# explicador KernelExplainer y con el DeepExplainer lo cual afecta a SVM poly 2 y 3, SVM Rbf y a MLP.\n",
    "# Dejo de todas formas implementado como sería con poly 2 por si alguien quiere probar su rendimiento, quizás \n",
    "# sea por falta de recursos, para el resto de los mencionados sería replicar simplemente.\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar:\n",
    "\n",
    "# shap_valores_globales(calibrated_svmLineal, 'svmLineal', top_20_feature_names)\n",
    "# shap_valores_globales(randomForest_modelDDOS, 'randomForest', top_20_feature_names)\n",
    "# shap_valores_globales(lgb_model_DDOS, 'lgb', top_20_feature_names)\n",
    "# shap_valores_globales(xgBoost_model_DDOS, 'xgBoost', top_20_feature_names)\n",
    "# shap_valores_globales(logisticRegression_model_DDOS, 'logisticRegression', top_20_feature_names)\n",
    "# shap_valores_globales(calibrated_svmPoly2, 'poly2', top_20_feature_names) #Problemas rendimiento KernelExplainer\n",
    "# shap_valores_globales(mlp_model_DDOS, 'mlp', top_20_feature_names) #Problemas rendimiento DeepExplainer\n",
    "\n",
    "# Function for SHAP global values\n",
    "def shap_valores_globales(model,modelName, feature_names, clase = 0):\n",
    "    shap.initjs()\n",
    "    if modelName == 'svmLineal':\n",
    "        try: \n",
    "            shap_values = joblib.load('shap_values_svmLineal.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_svmLineal = shap.LinearExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_svmLineal,'explainer_shap_svmLineal.pkl')\n",
    "            shap_values = explainer_shap_svmLineal.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_svmLineal.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "\n",
    "    elif modelName == 'randomForest':\n",
    "        try: \n",
    "            shap_values=joblib.load('shap_values_randomForest.pkl')\n",
    "            shap_values_class0 = shap_values[:,:,clase]\n",
    "            shap.summary_plot(shap_values_class0, X_test, feature_names = feature_names)\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_randomForest = shap.TreeExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_randomForest,'explainer_shap_randomForest.pkl')\n",
    "            shap_values = explainer_shap_randomForest.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_randomForest.pkl')\n",
    "            shap_values_class0 = shap_values[:,:,clase]\n",
    "            shap.summary_plot(shap_values_class0, X_test, feature_names = feature_names)\n",
    "        \n",
    "    elif modelName == 'lgb':\n",
    "        try: \n",
    "            shap_values = joblib.load('shap_values_lgb.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_lgb = shap.TreeExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_lgb,'explainer_shap_lgb.pkl')\n",
    "            shap_values = explainer_shap_lgb.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_lgb.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "    elif modelName == 'xgBoost':\n",
    "        try: \n",
    "            shap_values = joblib.load('shap_values_xgBoost.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_xgBoost = shap.TreeExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_xgBoost,'explainer_shap_xgBoost.pkl')\n",
    "            shap_values = explainer_shap_xgBoost.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_xgBoost.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "        \n",
    "    elif modelName == 'logisticRegression':\n",
    "        try:\n",
    "            shap_values = joblib.load('shap_values_logisticRegression.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_logisticRegression = shap.LinearExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_logisticRegression,'explainer_shap_logisticRegression.pkl')\n",
    "            shap_values = explainer_shap_logisticRegression.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_logisticRegression.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "\n",
    "    elif modelName == 'mlp':\n",
    "        try: \n",
    "            shap_values = joblib.load('shap_values_mlp.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_mlp = shap.DeepExplainer(model.predict_proba ,X_train_df) # si no lo soporta probar con KernelExplainer también\n",
    "            joblib.dump(explainer_shap_mlp,'explainer_shap_mlp.pkl')\n",
    "            shap_values = explainer_shap_mlp.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_mlp.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "        \n",
    "    elif modelName == 'poly2':\n",
    "        try: \n",
    "            shap_values = joblib.load('shap_values_poly2.pkl')\n",
    "            shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_svmPoly2 = shap.KernelExplainer(model.predict_proba, shap.sample(X_train, 100))\n",
    "            joblib.dump(explainer_shap_svmPoly2,'explainer_shap_poly2.pkl')\n",
    "            # Calcular los valores SHAP para todas las muestras de datos\n",
    "            shap_values = explainer_shap_svmPoly2.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_poly2.pkl')\n",
    "            # Mostrar un resumen de los valores SHAP\n",
    "            shap.summary_plot(shap_values, X_test_df, plot_type=\"bar\", feature_names=top_20_feature_names)\n",
    "    \n",
    "    else: \n",
    "        print(\"No se ha introducido un modelo válido\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPLICACION LOCAL PARA INSTANCIAS CON SHAP\n",
    "# Obtener una explicación interactiva para una muestra específica\n",
    "# Function for SHAP local explanation\n",
    "\n",
    "# IMPORTANTE haber ejecutado antes al menos una vez los valores SHAP y tener guardados los explicadores y valores SHAP.\n",
    "# Llamar a la función con el modelo , conjunto de test, instancia y nombres de características)\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar (con la instancia numero 1):\n",
    "\n",
    "# shap_explicacion_local('xgBoost', X_test, 1, top_20_feature_names)\n",
    "# shap_explicacion_local('randomForest', X_test, 1, top_20_feature_names)\n",
    "# shap_explicacion_local('svmLineal', X_test, 1, top_20_feature_names)\n",
    "# shap_explicacion_local('lgb', X_test, 1, top_20_feature_names)\n",
    "# shap_explicacion_local('logisticRegression', X_test, 1, top_20_feature_names)\n",
    "# shap_explicacion_local('mlp', X_test, 1, top_20_feature_names)\n",
    "# shap_explicacion_local('poly2', X_test, 1, top_20_feature_names)\n",
    "\n",
    "\n",
    "def shap_explicacion_local(explainerModelName, X_test, sample_index, feature_names):\n",
    "    explainerModel = joblib.load(f'explainer_shap_{explainerModelName}.pkl')\n",
    "    shap_values = joblib.load(f'shap_values_{explainerModelName}.pkl')\n",
    "    shap.initjs()\n",
    "    if explainerModelName == 'randomForest':\n",
    "        shap.force_plot(explainerModel.expected_value[0], shap_values[sample_index,:,1], X_test[sample_index,:], feature_names=feature_names,matplotlib=True,figsize=(100, 5))\n",
    "    elif explainerModelName == 'svmLineal' or explainerModelName == 'lgb' or explainerModelName == 'xgBoost' or explainerModelName == 'logisticRegression' or explainerModelName == 'mlp' or explainerModelName == 'poly2':\n",
    "        expl = explainerModel.shap_values(X_test[sample_index])\n",
    "        shap.force_plot(explainerModel.expected_value, expl, X_test[sample_index], feature_names = top_20_feature_names,matplotlib=True, figsize=(50, 2.5))\n",
    "    else: \n",
    "        print(\"No se ha introducido un modelo válido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RELACION DE VALORES SHAP CON VALORES REALES\n",
    "# Function for SHAP values vs real values\n",
    "\n",
    "# IMPORTANTE haber ejecutado antes al menos una vez los valores SHAP y tener guardados los explicadores y valores SHAP.\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar (con la característica 0 (van desde la 0-19)):\n",
    "\n",
    "# shap_valores_vs_reales('xgBoost', X_test_df, top_20_feature_names, 0)\n",
    "# shap_valores_vs_reales('lgb', X_test_df, top_20_feature_names, 0)\n",
    "# shap_valores_vs_reales('svmLineal', X_test_df, top_20_feature_names, 0)\n",
    "# shap_valores_vs_reales('randomForest', X_test_df, top_20_feature_names, 0, X) X puede ser 0 o 1 para la clase.\n",
    "# shap_valores_vs_reales('logisticRegression', X_test_df, top_20_feature_names, 0)\n",
    "# shap_valores_vs_reales('mlp', X_test_df, top_20_feature_names, 0)\n",
    "# shap_valores_vs_reales('poly2', X_test_df, top_20_feature_names, 0)\n",
    "\n",
    "def shap_valores_vs_reales(explainerModelName, X_test, feature_names, feature_index, clase=0):\n",
    "   if explainerModelName == 'randomForest':\n",
    "       # Choose an interaction index that is different from the feature index \n",
    "        shap_values = joblib.load(f'shap_values_{explainerModelName}.pkl')\n",
    "        shap.dependence_plot(feature_index, shap_values[:,:,clase], X_test, feature_names=feature_names,interaction_index=feature_names[feature_index])\n",
    "   elif explainerModelName == 'svmLineal' or explainerModelName =='lgb' or explainerModelName == 'xgBoost' or explainerModelName == 'logisticRegression' or explainerModelName == 'mlp' or explainerModelName == 'poly2':\n",
    "       # Choose an interaction index that is different from the feature index \n",
    "        shap_values = joblib.load(f'shap_values_{explainerModelName}.pkl')\n",
    "        shap.dependence_plot(feature_index, shap_values, X_test, feature_names=feature_names,interaction_index=feature_names[feature_index])\n",
    "   else: \n",
    "        print(\"No se ha introducido un modelo válido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE IMPORTANCIA DE CARACTERÍSTICAS PARA INSTANCIAS CONCRETAS CON LIME (GRAFICO DISPERSION)\n",
    "# Obtener las características y sus pesos\n",
    "\n",
    "# IMPORTANTE tener los modelos calibrados cargados y guardados.\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar (para la instancia 2):\n",
    "\n",
    "# lime_analisis_caracteristicasDispersion('logisticRegression', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicasDispersion('randomForest', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicasDispersion('lgb', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicasDispersion('xgBoost', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicasDispersion('mlp', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicasDispersion('svmLineal', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicasDispersion('svmPoly2', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicasDispersion('svmPoly3', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicasDispersion('svmRbf', X_train, X_test, 2, top_20_feature_names)\n",
    "\n",
    "def lime_analisis_caracteristicasDispersion(modelName, X_train, X_test, sample_index, feature_names):\n",
    "    if modelName == 'svmLineal' or modelName == 'svmPoly2' or modelName == 'svmPoly3' or modelName == 'svmRbf':\n",
    "        calibrated = joblib.load('calibrated_'f'{modelName}')\n",
    "    elif modelName == 'randomForest':\n",
    "        calibrated = randomForest_modelDDOS\n",
    "    elif modelName == 'lgb':\n",
    "        calibrated = lgb_model_DDOS\n",
    "    elif modelName == 'xgBoost':\n",
    "        calibrated = xgBoost_model_DDOS\n",
    "    elif modelName == 'logisticRegression':\n",
    "        calibrated = logisticRegression_model_DDOS\n",
    "    elif modelName == 'mlp':\n",
    "        calibrated = mlp_model_DDOS\n",
    "    else :\n",
    "        print(\"No se ha introducido un modelo válido\")\n",
    "        \n",
    "    explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['BENIGN', 'DDoS'], discretize_continuous=True)\n",
    "    expLime = explainer_lime.explain_instance(X_test[sample_index], calibrated.predict_proba)\n",
    "    weights = expLime.as_list()\n",
    "    feature_importance = {feat: weight for feat, weight in weights}\n",
    "    # Obtener las características y sus pesos\n",
    "    features = list(feature_importance.keys())\n",
    "    weights = list(feature_importance.values())\n",
    "\n",
    "    # Crear un gráfico de dispersión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(weights, features, color='green', alpha=0.5)\n",
    "    plt.xlabel('Peso')\n",
    "    plt.ylabel('Característica')\n",
    "    plt.title('Importancia de las características según LIME')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE IMPORTANCIA DE CARACTERÍSTICAS PARA INSTANCIAS CONCRETAS CON LIME \n",
    "# Obtener las características y sus pesos\n",
    "\n",
    "# IMPORTANTE tener los modelos calibrados cargados y guardados.\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar (para la instancia 2):\n",
    "\n",
    "# lime_analisis_caracteristicas('logisticRegression', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicas('randomForest', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicas('lgb', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicas('xgBoost', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicas('mlp', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicas('svmLineal', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicas('svmPoly2', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicas('svmPoly3', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_caracteristicas('svmRbf', X_train, X_test, 2, top_20_feature_names)\n",
    "\n",
    "\n",
    "def lime_analisis_caracteristicas(modelName, X_train, X_test, sample_index, feature_names):\n",
    "    if modelName == 'svmLineal' or modelName == 'svmPoly2' or modelName == 'svmPoly3' or modelName == 'svmRbf':\n",
    "        calibrated = joblib.load('calibrated_'f'{modelName}')\n",
    "    elif modelName == 'randomForest':\n",
    "        calibrated = randomForest_modelDDOS\n",
    "    elif modelName == 'lgb':\n",
    "        calibrated = lgb_model_DDOS\n",
    "    elif modelName == 'xgBoost':\n",
    "        calibrated = xgBoost_model_DDOS\n",
    "    elif modelName == 'logisticRegression':\n",
    "        calibrated = logisticRegression_model_DDOS\n",
    "    elif modelName == 'mlp':\n",
    "        calibrated = mlp_model_DDOS\n",
    "    else:\n",
    "        print(\"No se ha introducido un modelo válido\")\n",
    "        \n",
    "    explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['BENIGN', 'DDoS'], discretize_continuous=True)\n",
    "    expLime = explainer_lime.explain_instance(X_test[sample_index], calibrated.predict_proba)\n",
    "    weights = expLime.as_list()\n",
    "    feature_importance = {feat: weight for feat, weight in weights}\n",
    "    # Obtener las características y sus pesos\n",
    "    features = list(feature_importance.keys())\n",
    "    weights = list(feature_importance.values())\n",
    "\n",
    "    # Crear un gráfico de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(features, weights, color='skyblue')\n",
    "    plt.xlabel('Peso')\n",
    "    plt.ylabel('Característica')\n",
    "    plt.title('Importancia de las características según LIME')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPLICACION LOCAL PARA INSTANCIAS CON LIME\n",
    "# Function for LIME analysis for specific instances\n",
    "\n",
    "# IMPORTANTE tener los modelos calibrados cargados y guardados.\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar (para la instancia 2):\n",
    "\n",
    "# lime_analisis_instancias('logisticRegression', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_instancias('randomForest', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_instancias('lgb', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_instancias('xgBoost', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_instancias('mlp', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_instancias('svmLineal', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_instancias('svmPoly2', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_instancias('svmPoly3', X_train, X_test, 2, top_20_feature_names)\n",
    "# lime_analisis_instancias('svmRbf', X_train, X_test, 2, top_20_feature_names)\n",
    "\n",
    "def lime_analisis_instancias(modelName, X_train, X_test, sample_index, feature_names):\n",
    "    if modelName == 'svmLineal' or modelName == 'svmPoly2' or modelName == 'svmPoly3' or modelName == 'svmRbf':\n",
    "        calibrated = joblib.load('calibrated_'f'{modelName}')\n",
    "    elif modelName == 'randomForest':\n",
    "        calibrated = randomForest_modelDDOS\n",
    "    elif modelName == 'lgb':\n",
    "        calibrated = lgb_model_DDOS\n",
    "    elif modelName == 'xgBoost':\n",
    "        calibrated = xgBoost_model_DDOS\n",
    "    elif modelName == 'logisticRegression':\n",
    "        calibrated = logisticRegression_model_DDOS\n",
    "    elif modelName == 'mlp':\n",
    "        calibrated = mlp_model_DDOS\n",
    "    else:\n",
    "        print(\"No se ha introducido un modelo válido\")\n",
    "\n",
    "    explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['BENIGN', 'DDoS'], discretize_continuous=True)\n",
    "    exp = explainer_lime.explain_instance(X_test[sample_index], calibrated.predict_proba)\n",
    "    exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE RESIDUOS\n",
    "# Son la diferencia entre los valores reales y los valores predichos por el modelo\n",
    "# IMPORTANTE tener los modelos calibrados cargados y guardados.\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar:\n",
    "\n",
    "# analisis_residuos('xgBoost', X_test, y_test)\n",
    "# analisis_residuos('randomForest', X_test, y_test)\n",
    "# analisis_residuos('mlp', X_test, y_test)\n",
    "# analisis_residuos('lgb', X_test, y_test)\n",
    "# analisis_residuos('logisticRegression', X_test, y_test)\n",
    "# analisis_residuos('svmLineal', X_test, y_test)\n",
    "# analisis_residuos('svmPoly2', X_test, y_test)\n",
    "# analisis_residuos('svmPoly3', X_test, y_test)\n",
    "# analisis_residuos('svmRbf', X_test, y_test)\n",
    "\n",
    "def analisis_residuos(modelName,X_test,y_test):\n",
    "    # Copiar y_test para no modificar los datos originales\n",
    "\n",
    "    y_test_encoded = y_test.copy()\n",
    "\n",
    "    # Inicializar el codificador de etiquetas\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Ajustar el codificador de etiquetas y transformar las etiquetas\n",
    "    y_test_encoded = label_encoder.fit_transform(y_test_encoded)\n",
    "\n",
    "\n",
    "    if modelName == 'svmLineal' or modelName == 'svmPoly2' or modelName == 'svmPoly3' or modelName == 'svmRbf':\n",
    "        calibrated = joblib.load('calibrated_'f'{modelName}')\n",
    "    elif modelName == 'randomForest':\n",
    "        calibrated = randomForest_modelDDOS\n",
    "    elif modelName == 'lgb':\n",
    "        calibrated = lgb_model_DDOS\n",
    "    elif modelName == 'xgBoost':\n",
    "        calibrated = xgBoost_model_DDOS\n",
    "    elif modelName == 'mlp':\n",
    "        calibrated = mlp_model_DDOS\n",
    "    elif modelName == 'logisticRegression':\n",
    "        calibrated = logisticRegression_model_DDOS\n",
    "    else:\n",
    "        print(\"No se ha introducido un modelo válido\")\n",
    "        \n",
    "    # Realizar predicciones de probabilidad en lugar de clases\n",
    "    y_pred_prob = calibrated.predict_proba(X_test)[:, 1]  # Probabilidad de pertenencia a la clase positiva\n",
    "\n",
    "    # Calcular los residuos\n",
    "    residuos = y_test_encoded - y_pred_prob\n",
    "\n",
    "    # Visualizar los residuos\n",
    "    plt.scatter(y_pred_prob, residuos)\n",
    "    plt.xlabel('Probabilidad Predicha (Clase Positiva)')\n",
    "    plt.ylabel('Residuos')\n",
    "    plt.title('Análisis de Residuos para Modelo ' f'{modelName}')\n",
    "    plt.axhline(y=0, color='r', linestyle='-')  # Línea horizontal en y=0 para referencia\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Crear un histograma de los residuos\n",
    "    plt.hist(residuos, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Residuos')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title('Histograma de Residuos para Modelo ' f'{modelName}')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')  # Línea vertical en x=0 para referencia\n",
    "    plt.show()\n",
    "\n",
    "    # Crear un gráfico de densidad de kernel de los residuos\n",
    "    sns.kdeplot(residuos, shade=True)\n",
    "    plt.xlabel('Residuos')\n",
    "    plt.ylabel('Densidad de Probabilidad')\n",
    "    plt.title('Densidad de Kernel de Residuos para Modelo ' f'{modelName}')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')  # Línea vertical en x=0 para referencia\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE SENSIBILIDAD\n",
    "# La gráfica que se genera muestra cómo varía la predicción media del modelo SVM lineal \n",
    "# a medida que se ajusta el valor de la característica. \n",
    "\n",
    "# IMPORTANTE tener los modelos cargados y guardados.\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar (en este caso con la característica número 16 (van desde la 0-19)):\n",
    "\n",
    "# analisis_sensibilidad(xgBoost_model_DDOS, X_test, 16)\n",
    "# analisis_sensibilidad(svm_model_linearDDOS, X_test, 16)\n",
    "# analisis_sensibilidad(svm_model_poly_degree_2DDOS, X_test, 16)\n",
    "# analisis_sensibilidad(svm_model_poly_degree_3DDOS, X_test, 16)\n",
    "# analisis_sensibilidad(svm_model_rbfDDOS, X_test, 16)\n",
    "# analisis_sensibilidad(mlp_model_DDOS, X_test, 16)\n",
    "# analisis_sensibilidad(lgb_model_DDOS, X_test, 16)\n",
    "# analisis_sensibilidad(randomForest_modelDDOS, X_test, 16)\n",
    "# analisis_sensibilidad(logisticRegression_model_DDOS, X_test, 16)\n",
    "\n",
    "def analisis_sensibilidad(model, X_test, feature_idx):\n",
    "\n",
    "    valores_caracteristica = np.linspace(X_test[:, feature_idx].min(), X_test[:, feature_idx].max(), 100)\n",
    "    X_test_mean = X_test.mean(axis=0)\n",
    "\n",
    "    # Inicializar una lista para almacenar las predicciones\n",
    "    predicciones = []\n",
    "\n",
    "    # Realizar predicciones para cada valor de la característica\n",
    "    for valor in valores_caracteristica:\n",
    "        X_test_modificado = X_test.copy()\n",
    "        X_test_modificado[:, feature_idx] = valor\n",
    "        \n",
    "        # Realizar las predicciones y asegurarse de que sean numéricas\n",
    "        predicciones_modelo = model.predict(X_test_modificado)\n",
    "        \n",
    "        # Calcular la media de las predicciones (asegurándose de que sean valores numéricos)\n",
    "        prediccion_media = np.mean([1 if pred == 'DDoS' or pred == 1 else 0 for pred in predicciones_modelo])\n",
    "        predicciones.append(prediccion_media)\n",
    "\n",
    "    # Visualizar el cambio en las predicciones\n",
    "    plt.plot(valores_caracteristica, predicciones)\n",
    "    feature_name = top_20_feature_names[feature_idx]\n",
    "    plt.xlabel('Valor de ' f'{feature_name}')\n",
    "    plt.ylabel('Predicción Media')\n",
    "    plt.title('Análisis de Sensibilidad')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE INFLUENCIA\n",
    "\n",
    "# Calcula exactitud, precisión, Recall y F1 Score del modelo , para el conjunto de datos de test entero y después \n",
    "# para una sola parte del conjunto y se comparan en la gráfica.\n",
    "\n",
    "# IMPORTANTE tener los modelos cargados y guardados.\n",
    "\n",
    "# Anotación: para XGBoost se utiliza el y_test_encoded ya que predice con 0-1 no con cadenas como DDoS-Benign\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar (en este caso un rango de exclusion de 18000-35500, se puede elegir el rango que se quiera\n",
    "# siempre cuando esté dentro de los límites hasta 45143):\n",
    "\n",
    "# analisis_influencia(svm_model_poly_degree_2DDOS,'svmPoly2', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(svm_model_linearDDOS,'svmLineal', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(svm_model_poly_degree_3DDOS,'svmPoly3', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(svm_model_rbfDDOS,'svmRbf', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(mlp_model_DDOS,'mlp', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(lgb_model_DDOS,'lgb', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(randomForest_modelDDOS,'randomForest', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(logisticRegression_model_DDOS,'logisticRegression', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(logisticRegression_model_DDOS,'xgBoost', X_test_df, y_test, slice(18000, 35500))\n",
    "# analisis_influencia(xgBoost_model_DDOS,'xgBoost', X_test_df, y_test_encoded, slice(18000, 35500))\n",
    "\n",
    "def analisis_influencia (model, modelName, X_test, y_test, range_exclusion):\n",
    "    # Realizar predicciones con el conjunto de pruebas original\n",
    "    if modelName == 'xgBoost':\n",
    "        pos_label = 1\n",
    "    else:\n",
    "        pos_label = 'DDoS'\n",
    "    \n",
    "    predicciones_original = model.predict(X_test)\n",
    "\n",
    "    # Calcular las métricas de rendimiento\n",
    "    accuracy_original = accuracy_score(y_test, predicciones_original)\n",
    "    precision_original = precision_score(y_test, predicciones_original, pos_label=pos_label)\n",
    "    recall_original = recall_score(y_test, predicciones_original, pos_label=pos_label)\n",
    "    f1_original = f1_score(y_test, predicciones_original, pos_label=pos_label)\n",
    "\n",
    "    # Excluir las primeras 20000 instancias del conjunto de pruebas\n",
    "    X_test_excluido = np.delete(X_test, range_exclusion, axis=0)\n",
    "    y_test_excluido = np.delete(y_test, range_exclusion)\n",
    "\n",
    "    # Realizar predicciones con el conjunto de pruebas excluyendo las primeras 20000 instancias\n",
    "    predicciones_excluido = model.predict(X_test_excluido)\n",
    "\n",
    "    # Calcular las métricas de rendimiento nuevamente\n",
    "    accuracy_excluido = accuracy_score(y_test_excluido, predicciones_excluido)\n",
    "    precision_excluido = precision_score(y_test_excluido, predicciones_excluido, pos_label=pos_label)\n",
    "    recall_excluido = recall_score(y_test_excluido, predicciones_excluido, pos_label=pos_label)\n",
    "    f1_excluido = f1_score(y_test_excluido, predicciones_excluido, pos_label=pos_label)\n",
    "\n",
    "    # Comparar las métricas antes y después de la exclusión\n",
    "    metricas = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    valores_original = [accuracy_original, precision_original, recall_original, f1_original]\n",
    "    valores_excluido = [accuracy_excluido, precision_excluido, recall_excluido, f1_excluido]\n",
    "\n",
    "    # Visualización de la comparación de métricas\n",
    "    x = np.arange(len(metricas))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, valores_original, width, label='Original')\n",
    "    rects2 = ax.bar(x + width/2, valores_excluido, width, label='Excluido')\n",
    "\n",
    "    # Añadir etiquetas, título y leyenda\n",
    "    ax.set_xlabel('Métricas')\n",
    "    ax.set_ylabel('Valores')\n",
    "    ax.set_title('Comparación de Métricas de Rendimiento antes y después de la Exclusión')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metricas)\n",
    "    ax.legend()\n",
    "\n",
    "    # Añadir valores encima de las barras\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('%.2f' % height,\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 puntos de offset vertical\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PERMUTACION DE CARACTERÍSTICAS\n",
    "# Permuta los valores de una característica pasada por parametro, de forma aleatoria y se comparan\n",
    "# las precisiones del modelo antes de la permutación y después de esta.\n",
    "\n",
    "# IMPORTANTE tener los modelos cargados y guardados.\n",
    "\n",
    "# Anotación: para XGBoost se utiliza el y_test_encoded ya que predice con 0-1 no con cadenas como DDoS-Benign\n",
    "\n",
    "# Ejemplos de como se llamaría a la función con cada uno de los modelos para copiar y pegar en la última celda el\n",
    "# que se quiera ejecutar (en este caso para la característica número 0 (van desde la 0-19)):\n",
    "\n",
    "# permutacion_de_caracteristicas(xgBoost_model_DDOS, X_test, y_test_encoded, 0)\n",
    "# permutacion_de_caracteristicas(svm_model_linearDDOS, X_test, y_test, 0)\n",
    "# permutacion_de_caracteristicas(svm_model_poly_degree_2DDOS, X_test, y_test, 0)\n",
    "# permutacion_de_caracteristicas(svm_model_poly_degree_3DDOS, X_test, y_test, 0)\n",
    "# permutacion_de_caracteristicas(svm_model_rbfDDOS, X_test, y_test, 0)\n",
    "# permutacion_de_caracteristicas(lgb_model_DDOS, X_test, y_test, 0) \n",
    "# permutacion_de_caracteristicas(mlp_model_DDOS, X_test, y_test, 0) \n",
    "# permutacion_de_caracteristicas(logisticRegression_model_DDOS, X_test, y_test, 0)\n",
    "# permutacion_de_caracteristicas(randomForest_modelDDOS, X_test, y_test, 0) \n",
    "\n",
    "def permutacion_de_caracteristicas(model, X_test, y_test, feature):\n",
    "    precision_original = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"Precisión original:\", precision_original)\n",
    "\n",
    "    # Función para calcular la precisión después de permutar una característica específica\n",
    "    def precision_despues_permutacion(X, y, modelo, caracteristica_permutada):\n",
    "        X_permutado = X.copy()\n",
    "        np.random.shuffle(X_permutado[:, caracteristica_permutada])  # Permutar los valores de la característica\n",
    "        return accuracy_score(y, modelo.predict(X_permutado))\n",
    "\n",
    "    # Calcular la precisión después de permutar la característica seleccionada\n",
    "    precision_permutacion = precision_despues_permutacion(X_test, y_test, model, feature)\n",
    "    print(\"Precisión después de la permutación de la característica:\", precision_permutacion)\n",
    "\n",
    "    # Calcular el cambio en la precisión debido a la permutación de características\n",
    "    cambio_precision = precision_permutacion - precision_original\n",
    "    print(\"Cambio en la precisión:\", cambio_precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda para ejecutar las distintas funciones, ya hay ejemplos de las últimas ejecuciones, si no en cada celda\n",
    "# de cada función hay un ejemplo para cada modelo, basta con copiar y pegar aquí y si se quiere cambiar algún\n",
    "# parametro como la característica, instancia etc.\n",
    "\n",
    "#explicabilidad_global(logisticRegression_model_DDOS, top_20_feature_names) \n",
    "#analisis_residuos('svmLineal', X_test, y_test)\n",
    "#analisis_sensibilidad(xgBoost_model_DDOS, X_test, 16) \n",
    "#analisis_influencia(xgBoost_model_DDOS,'xgBoost', X_test_df, y_test_encoded, slice(18000, 35500)) \n",
    "#permutacion_de_caracteristicas(randomForest_modelDDOS, X_test, y_test, 0) \n",
    "#shap_valores_globales(mlp_model_DDOS, 'mlp', top_20_feature_names)\n",
    "#shap_values_randomForest = joblib.load('shap_values_randomForest.pkl')\n",
    "#shap_valores_globales(randomForest_modelDDOS, 'randomForest', top_20_feature_names)\n",
    "#shap_valores_globales(mlp_model_DDOS, 'mlp', top_20_feature_names)\n",
    "#shap_explicacion_local('xgBoost', X_test, 1, top_20_feature_names) \n",
    "#shap_valores_vs_reales('svmLineal', X_test_df, top_20_feature_names, 0) \n",
    "#lime_analisis_instancias('svmLineal', X_train, X_test, sample_index=0, feature_names=top_20_feature_names) \n",
    "#lime_analisis_caracteristicas('logisticRegression', X_train, X_test, sample_index=2, feature_names=top_20_feature_names) \n",
    "#lime_analisis_caracteristicasDispersion('logisticRegression', X_train, X_test, sample_index=2, feature_names=top_20_feature_names) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
