{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javier\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importación de librerías necesarias para el procesamiento de datos y análisis\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import sklearn\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARGA DE MODELOS\n",
    "\n",
    "def cargar_datos():\n",
    "    # Verificar si los archivos con los datos ya existen\n",
    "    \n",
    "    if os.path.exists('datos_X_PortScan.csv') and os.path.exists('datos_Y_PortScan.csv'):\n",
    "        X = pd.read_csv('datos_X_PortScan.csv')\n",
    "       # y = pd.read_csv('datos_y.csv', header=None, squeeze=True)  # Usar squeeze=True para obtener una Serie\n",
    "        y = pd.read_csv('datos_Y_PortScan.csv', header=None, skiprows=1).iloc[:, 0]  # Obtener la Serie de una sola columna\n",
    "    else:\n",
    "        # Recopilación de archivos CSV de un directorio y concatenación en un DataFrame\n",
    "        csv_files = []\n",
    "        for dirname, _, filenames in os.walk('dataset\\MachineLearningCSV_reducido\\MachineLearningCVE-PortScan'):\n",
    "            for filename in filenames:\n",
    "                csv_file = os.path.join(dirname, filename)\n",
    "                print(os.path.join(dirname, filename))\n",
    "                csv_files.append(csv_file)\n",
    "        df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "        # Limpieza de datos: eliminación de espacios en nombres de columnas y valores nulos o infinitos\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Preprocesamiento de la columna 'Label' para agrupar categorías de ataques similares\n",
    "        df_experiment = df.copy(deep=True)\n",
    "        df_experiment.Label.replace(\"Web.*\", \"Web Attack\", regex=True, inplace=True)\n",
    "        df_experiment.Label.replace(r'.*Patator$', \"Brute Force\", regex=True,inplace=True)\n",
    "        df_experiment.Label.replace([\"DoS GoldenEye\", \"DoS Hulk\", \"DoS Slowhttptest\", \"DoS slowloris\"], \"DoS\", inplace=True)\n",
    "\n",
    "        # División de los datos en conjuntos de entrenamiento y prueba\n",
    "        y = df_experiment.Label\n",
    "        X = df_experiment.drop(columns='Label')\n",
    "\n",
    "        # Guardar los datos en archivos CSV\n",
    "        X.to_csv('datos_X_PortScan.csv', index=False)\n",
    "        y.to_csv('datos_Y_PortScan.csv', index=False)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Definir una función para realizar validación cruzada y obtener métricas de rendimiento\n",
    "def cross_validate_model(model, X, y,model_name,cv=5):\n",
    "    try: \n",
    "        scores = joblib.load(f'{model_name}_cross_val_results.pkl')      \n",
    "    except FileNotFoundError:\n",
    "        scores = cross_val_score(model, X, y, cv=cv)\n",
    "        joblib.dump(scores,f'{model_name}_cross_val_results.pkl')\n",
    "\n",
    "    print(\"Puntuaciones de validación cruzada:\", scores)\n",
    "    print(\"Media de puntuaciones:\", scores.mean())\n",
    "    print(\"Desviación estándar de puntuaciones:\", scores.std())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, X_selected, y, model_name):\n",
    "    # Realizar predicciones y calcular la puntuación del modelo\n",
    "    try: \n",
    "        score = joblib.load(f'{model_name}_score.pkl')\n",
    "    except FileNotFoundError:\n",
    "        score = model.score(X_test, y_test)\n",
    "        joblib.dump(score,f'{model_name}_score.pkl')\n",
    "\n",
    "    print(f\"Puntuación del {model_name}: {score}\") \n",
    "    print(f\"Validación cruzada para {model_name}:\")\n",
    "    cross_validate_model(model, X_selected, y, model_name)\n",
    "    mostrar_ejemplos_predicciones(model, model_name, X_test, y_test)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "def test_svm_models(X_train, X_test, y_train, y_test, kernel, degree=0):\n",
    "\n",
    "    if kernel == 'poly':\n",
    "        svm_model = SVC(kernel=kernel, degree=degree)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "        joblib.dump(svm_model, f'svm_model_{kernel}_degree_{degree}PortScan.pkl')\n",
    "        score = svm_model.score(X_test, y_test)\n",
    "        print(f\"Score del SVM con kernel {kernel}: {score:.4f}: \")\n",
    "    else:\n",
    "        svm_model = SVC(kernel=kernel)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "        joblib.dump(svm_model, f'svm_model_{kernel}PortScan.pkl')\n",
    "        score = svm_model.score(X_test, y_test)\n",
    "        print(f\"Score del SVM con kernel {kernel}: {score:.4f}: \")\n",
    "\n",
    "\n",
    "# Función para mostrar ejemplos de predicciones\n",
    "def mostrar_ejemplos_predicciones(modelo, modelo_name,X_test, y_test, num_ejemplos=5):\n",
    "    predicciones = modelo.predict(X_test)\n",
    "    print(\"\\nMatriz de confusión del modelo: \"+f'{modelo_name}')\n",
    "    print(confusion_matrix(y_test, predicciones))\n",
    "    print(\"\\nInforme de clasificación del modelo: \"+f'{modelo_name}')\n",
    "    print(classification_report(y_test, predicciones))\n",
    "\n",
    "    print(\"Ejemplos de predicciones:\")\n",
    "    for i in range(num_ejemplos):\n",
    "        ejemplo_prediccion = f\"Ejemplo {i+1}: Predicción={predicciones[i]}, Verdadero={y_test.iloc[i]}\"\n",
    "        print(ejemplo_prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n",
      "svmLineal\n",
      "poly2\n",
      "poly3\n",
      "rbf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'try: \\n    kmeansAndLGBM_model_DDOS = joblib.load(\\'kmeansAndLGBM_model_DDOS.pkl\\')\\n    # SHAP para LGBM\\n    explainer_shap = shap.TreeExplainer(kmeansAndLGBM_model_DDOS,X_train_df)\\n    shap_values = explainer_shap.shap_values(X_test_df, check_additivity=False)\\n\\n    # Visualización de los valores SHAP\\n    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\\n\\n    #evaluate_model(kmeansAndLGBM_model_DDOS, X_test, y_test_encoded_series, X_scaled, y_encoded, \"kmeansAndLGBM_model\")\\nexcept FileNotFoundError:\\n    kmeans = KMeans(n_clusters=2, random_state=42)\\n    cluster_labels = kmeans.fit_predict(X_train)\\n    # Entrenar un modelo de clasificación con las etiquetas del clustering\\n    kmeansAndLGBM_model_DDOS = LGBMClassifier(verbosity=-1)\\n    kmeansAndLGBM_model_DDOS.fit(X_train, cluster_labels)\\n    joblib.dump(kmeansAndLGBM_model_DDOS,\\'kmeansAndLGBM_model_DDOS.pkl\\')'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CARGA DE DATOS\n",
    "\n",
    "X,y = cargar_datos()\n",
    "\n",
    "\n",
    "try: \n",
    "    X_scaled = joblib.load('X_scaled.pkl')\n",
    "    top_20_feature_names = joblib.load('top_20_feature_names.pkl')\n",
    "except FileNotFoundError:\n",
    "    indices = joblib.load('indices.pkl')\n",
    "    top_20_feature_indices = indices[:20]  # Índices de las 20 características más importantes\n",
    "    top_20_feature_names = X.columns[top_20_feature_indices].tolist() # Nombres de las 20 características más importantes\n",
    "    joblib.dump(top_20_feature_names,'top_20_feature_names.pkl')\n",
    "    X_selected = X.values[:, indices[:20]]  # Seleccionar las 20 características más importantes\n",
    "    # Normalizar los datos\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    joblib.dump(X_scaled,'X_scaled.pkl')\n",
    "\n",
    "\n",
    "\n",
    "try: \n",
    "    X_train = joblib.load('X_train.pkl')\n",
    "    X_test = joblib.load('X_test.pkl')\n",
    "    y_train = joblib.load('y_train.pkl')\n",
    "    y_test = joblib.load('y_test.pkl')\n",
    "    X_train_df = pd.DataFrame(X_train, columns=top_20_feature_names)\n",
    "    X_test_df = pd.DataFrame(X_test, columns=top_20_feature_names)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    joblib.dump(X_train,'X_train.pkl')\n",
    "    joblib.dump(X_test,'X_test.pkl')\n",
    "    joblib.dump(y_train,'y_train.pkl')\n",
    "    joblib.dump(y_test,'y_test.pkl')\n",
    "\n",
    "try:\n",
    "    randomForest_modelPortScan = joblib.load('randomForest_model_PortScan.pkl')\n",
    "    print(\"random\")\n",
    "except FileNotFoundError:\n",
    "    randomForest_modelDDOS = RandomForestClassifier()\n",
    "    randomForest_modelDDOS.fit(X_train, y_train)\n",
    "    joblib.dump(randomForest_modelDDOS, 'randomForest_model_PortScan.pkl')\n",
    "\n",
    "try:\n",
    "    svm_model_linearPortScan = joblib.load('svm_model_linearPortScan.pkl')\n",
    "    print(\"svmLineal\")\n",
    "except FileNotFoundError:\n",
    "    test_svm_models(X_train, X_test, y_train, y_test, \"linear\")\n",
    "    print(\"svmLineal\")\n",
    "\n",
    "try:\n",
    "    svm_model_poly_degree_2PortScan = joblib.load('svm_model_poly_degree_2PortScan.pkl')\n",
    "    print(\"poly2\")\n",
    "except FileNotFoundError:\n",
    "    test_svm_models(X_train, X_test, y_train, y_test, \"poly\", 2)\n",
    "    print(\"poly2\")\n",
    "\n",
    "try:\n",
    "    svm_model_poly_degree_3PortScan = joblib.load('svm_model_poly_degree_3PortScan.pkl')\n",
    "    print(\"poly3\")\n",
    "except FileNotFoundError:\n",
    "    test_svm_models(X_train, X_test, y_train, y_test, \"poly\", 3)\n",
    "    print(\"poly3\")\n",
    "\n",
    "try:\n",
    "    svm_model_rbfPortScan = joblib.load('svm_model_rbfPortScan.pkl')\n",
    "    print(\"rbf\")\n",
    "except FileNotFoundError:\n",
    "    test_svm_models(X_train, X_test, y_train, y_test, \"rbf\")\n",
    "    print(\"rbf\")\n",
    "\n",
    "try: \n",
    "    lgb_model_PortScan = joblib.load('lgb_model_PortScan.pkl')\n",
    "    #evaluate_model(lgb_model_DDOS, X_test, y_test, X_scaled, y, \"lgbModel\")\n",
    "except FileNotFoundError:\n",
    "    lgb_model = lgb.LGBMClassifier(verbosity=-1)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    joblib.dump(lgb_model, 'lgb_model_PortScan.pkl')\n",
    "    print(\"lgb\")\n",
    "\n",
    "try: \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    y_encoded = label_encoder.transform(y)\n",
    "    y_test_encoded_series = pd.Series(y_test_encoded)\n",
    "    xgBoost_model_PortScan = joblib.load('xgBoost_model_PortScan.pkl')\n",
    "    #evaluate_model(xgBoost_model_DDOS, X_test, y_test_encoded_series, X_scaled, y_encoded, \"xgBoost\")\n",
    "except FileNotFoundError:\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    xgBoost_model = xgb.XGBClassifier()\n",
    "    xgBoost_model.fit(X_train, y_train_encoded)\n",
    "    joblib.dump(xgBoost_model, 'xgBoost_model_PortScan.pkl')\n",
    "    print(\"xgboost\")\n",
    "\n",
    "try: \n",
    "    logisticRegression_model_PortScan = joblib.load('logisticRegression_model_PortScan.pkl')\n",
    "    #evaluate_model(logisticRegression_model_DDOS, X_test, y_test, X_scaled, y, \"logistic\")\n",
    "except FileNotFoundError:\n",
    "    # Entrenamiento del modelo\n",
    "    logistic_model = LogisticRegression(penalty='l2', solver='saga', max_iter=10000)\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    joblib.dump(logistic_model, 'logisticRegression_model_PortScan.pkl')\n",
    "    print(\"logistic\")\n",
    "\n",
    "try: \n",
    "    mlp_model_PortScan = joblib.load('mlp_model_PortScan.pkl')\n",
    "    #evaluate_model(mlp_model_DDOS,X_test,y_test,X_scaled,y,\"mlp_model\")\n",
    "except FileNotFoundError:\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, solver='adam', random_state=42)\n",
    "    mlp_model.fit(X_train,y_train)\n",
    "    joblib.dump(mlp_model,'mlp_model_PortScan.pkl')\n",
    "    print(\"mlp\")\n",
    "\n",
    "'''try: \n",
    "    kmeansAndLGBM_model_DDOS = joblib.load('kmeansAndLGBM_model_DDOS.pkl')\n",
    "    # SHAP para LGBM\n",
    "    explainer_shap = shap.TreeExplainer(kmeansAndLGBM_model_DDOS,X_train_df)\n",
    "    shap_values = explainer_shap.shap_values(X_test_df, check_additivity=False)\n",
    "\n",
    "    # Visualización de los valores SHAP\n",
    "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "    #evaluate_model(kmeansAndLGBM_model_DDOS, X_test, y_test_encoded_series, X_scaled, y_encoded, \"kmeansAndLGBM_model\")\n",
    "except FileNotFoundError:\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_train)\n",
    "    # Entrenar un modelo de clasificación con las etiquetas del clustering\n",
    "    kmeansAndLGBM_model_DDOS = LGBMClassifier(verbosity=-1)\n",
    "    kmeansAndLGBM_model_DDOS.fit(X_train, cluster_labels)\n",
    "    joblib.dump(kmeansAndLGBM_model_DDOS,'kmeansAndLGBM_model_DDOS.pkl')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas únicas y cantidad de clases:\n",
      "labels: ['BENIGN' 'PortScan']\n",
      "classes 2\n",
      "Puntuación del randomForest: 0.9998776651520448\n",
      "Validación cruzada para randomForest:\n",
      "Puntuaciones de validación cruzada: [0.999633   0.99968542 0.99944075 0.99984271 0.99877663]\n",
      "Media de puntuaciones: 0.9994756999143799\n",
      "Desviación estándar de puntuaciones: 0.00037238166967308166\n",
      "\n",
      "Matriz de confusión del modelo: randomForest\n",
      "[[25455     4]\n",
      " [    3 31758]]\n",
      "\n",
      "Informe de clasificación del modelo: randomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     25459\n",
      "    PortScan       1.00      1.00      1.00     31761\n",
      "\n",
      "    accuracy                           1.00     57220\n",
      "   macro avg       1.00      1.00      1.00     57220\n",
      "weighted avg       1.00      1.00      1.00     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 2: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Ejemplo 3: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 4: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 5: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Puntuación del logisticRegression: 0.9559245019224047\n",
      "Validación cruzada para logisticRegression:\n",
      "Puntuaciones de validación cruzada: [0.94694163 0.96212796 0.95178175 0.95705972 0.95506737]\n",
      "Media de puntuaciones: 0.9545956869514365\n",
      "Desviación estándar de puntuaciones: 0.005089832683443157\n",
      "\n",
      "Matriz de confusión del modelo: logisticRegression\n",
      "[[23263  2196]\n",
      " [  326 31435]]\n",
      "\n",
      "Informe de clasificación del modelo: logisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       0.99      0.91      0.95     25459\n",
      "    PortScan       0.93      0.99      0.96     31761\n",
      "\n",
      "    accuracy                           0.96     57220\n",
      "   macro avg       0.96      0.95      0.96     57220\n",
      "weighted avg       0.96      0.96      0.96     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 2: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Ejemplo 3: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 4: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 5: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Puntuación del mlp: 0.999580566235582\n",
      "Validación cruzada para mlp:\n",
      "Puntuaciones de validación cruzada: [0.99951066 0.99874168 0.99917859 0.99958056 0.99806008]\n",
      "Media de puntuaciones: 0.9990143151377667\n",
      "Desviación estándar de puntuaciones: 0.0005619369238820091\n",
      "\n",
      "Matriz de confusión del modelo: mlp\n",
      "[[25438    21]\n",
      " [    3 31758]]\n",
      "\n",
      "Informe de clasificación del modelo: mlp\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     25459\n",
      "    PortScan       1.00      1.00      1.00     31761\n",
      "\n",
      "    accuracy                           1.00     57220\n",
      "   macro avg       1.00      1.00      1.00     57220\n",
      "weighted avg       1.00      1.00      1.00     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 2: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Ejemplo 3: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 4: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 5: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Puntuación del lgb: 0.999790283117791\n",
      "Validación cruzada para lgb:\n",
      "Puntuaciones de validación cruzada: [0.99958057 0.99965047 0.99942327 0.99986019 0.99870672]\n",
      "Media de puntuaciones: 0.9994442420155325\n",
      "Desviación estándar de puntuaciones: 0.0003945873616757692\n",
      "\n",
      "Matriz de confusión del modelo: lgb\n",
      "[[25451     8]\n",
      " [    4 31757]]\n",
      "\n",
      "Informe de clasificación del modelo: lgb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     25459\n",
      "    PortScan       1.00      1.00      1.00     31761\n",
      "\n",
      "    accuracy                           1.00     57220\n",
      "   macro avg       1.00      1.00      1.00     57220\n",
      "weighted avg       1.00      1.00      1.00     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 2: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Ejemplo 3: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 4: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 5: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Puntuación del xgBoost: 0.999790283117791\n",
      "Validación cruzada para xgBoost:\n",
      "Puntuaciones de validación cruzada: [0.99961552 0.99966794 0.99944075 0.99986019 0.99877663]\n",
      "Media de puntuaciones: 0.9994722046330097\n",
      "Desviación estándar de puntuaciones: 0.00037261123629208265\n",
      "\n",
      "Matriz de confusión del modelo: xgBoost\n",
      "[[25450     9]\n",
      " [    3 31758]]\n",
      "\n",
      "Informe de clasificación del modelo: xgBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     25459\n",
      "           1       1.00      1.00      1.00     31761\n",
      "\n",
      "    accuracy                           1.00     57220\n",
      "   macro avg       1.00      1.00      1.00     57220\n",
      "weighted avg       1.00      1.00      1.00     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=1, Verdadero=1\n",
      "Ejemplo 2: Predicción=0, Verdadero=0\n",
      "Ejemplo 3: Predicción=1, Verdadero=1\n",
      "Ejemplo 4: Predicción=1, Verdadero=1\n",
      "Ejemplo 5: Predicción=0, Verdadero=0\n",
      "Puntuación del svmLineal: 0.9754281719678434\n",
      "Validación cruzada para svmLineal:\n",
      "Puntuaciones de validación cruzada: [0.97708843 0.98372918 0.97385484 0.974484   0.97334801]\n",
      "Media de puntuaciones: 0.9765008927509022\n",
      "Desviación estándar de puntuaciones: 0.003836755276360043\n",
      "\n",
      "Matriz de confusión del modelo: svmLineal\n",
      "[[24306  1153]\n",
      " [  253 31508]]\n",
      "\n",
      "Informe de clasificación del modelo: svmLineal\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       0.99      0.95      0.97     25459\n",
      "    PortScan       0.96      0.99      0.98     31761\n",
      "\n",
      "    accuracy                           0.98     57220\n",
      "   macro avg       0.98      0.97      0.98     57220\n",
      "weighted avg       0.98      0.98      0.98     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 2: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Ejemplo 3: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 4: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 5: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Puntuación del svmPoly2: 0.9505068157986718\n",
      "Validación cruzada para svmPoly2:\n",
      "Puntuaciones de validación cruzada: [0.93516253 0.94374246 0.94604939 0.94770968 0.94049179]\n",
      "Media de puntuaciones: 0.9426311708959242\n",
      "Desviación estándar de puntuaciones: 0.004451245405177226\n",
      "\n",
      "Matriz de confusión del modelo: svmPoly2\n",
      "[[22701  2758]\n",
      " [   74 31687]]\n",
      "\n",
      "Informe de clasificación del modelo: svmPoly2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      0.89      0.94     25459\n",
      "    PortScan       0.92      1.00      0.96     31761\n",
      "\n",
      "    accuracy                           0.95     57220\n",
      "   macro avg       0.96      0.94      0.95     57220\n",
      "weighted avg       0.95      0.95      0.95     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 2: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Ejemplo 3: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 4: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 5: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Puntuación del svmPoly3: 0.9354596295001748\n",
      "Validación cruzada para svmPoly3:\n",
      "Puntuaciones de validación cruzada: [0.92429221 0.92962128 0.93112428 0.93374578 0.92696482]\n",
      "Media de puntuaciones: 0.9291496730379576\n",
      "Desviación estándar de puntuaciones: 0.003274577316240489\n",
      "\n",
      "Matriz de confusión del modelo: svmPoly3\n",
      "[[21917  3542]\n",
      " [  151 31610]]\n",
      "\n",
      "Informe de clasificación del modelo: svmPoly3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       0.99      0.86      0.92     25459\n",
      "    PortScan       0.90      1.00      0.94     31761\n",
      "\n",
      "    accuracy                           0.94     57220\n",
      "   macro avg       0.95      0.93      0.93     57220\n",
      "weighted avg       0.94      0.94      0.93     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 2: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Ejemplo 3: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 4: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 5: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Puntuación del svmRbf: 0.9930269136665502\n",
      "Validación cruzada para svmRbf:\n",
      "Puntuaciones de validación cruzada: [0.99341139 0.99370838 0.99192576 0.99398801 0.99078977]\n",
      "Media de puntuaciones: 0.9927646641451455\n",
      "Desviación estándar de puntuaciones: 0.0012173251505768765\n",
      "\n",
      "Matriz de confusión del modelo: svmRbf\n",
      "[[25122   337]\n",
      " [   62 31699]]\n",
      "\n",
      "Informe de clasificación del modelo: svmRbf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      0.99      0.99     25459\n",
      "    PortScan       0.99      1.00      0.99     31761\n",
      "\n",
      "    accuracy                           0.99     57220\n",
      "   macro avg       0.99      0.99      0.99     57220\n",
      "weighted avg       0.99      0.99      0.99     57220\n",
      "\n",
      "Ejemplos de predicciones:\n",
      "Ejemplo 1: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 2: Predicción=BENIGN, Verdadero=BENIGN\n",
      "Ejemplo 3: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 4: Predicción=PortScan, Verdadero=PortScan\n",
      "Ejemplo 5: Predicción=BENIGN, Verdadero=BENIGN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Etiquetas únicas y cantidad de clases:\")\n",
    "labels = y.unique()\n",
    "classes = y.nunique()\n",
    "print(\"labels:\", labels) \n",
    "print(\"classes\", classes)\n",
    "evaluate_model(randomForest_modelPortScan, X_test, y_test, X_scaled, y, \"randomForest\")\n",
    "evaluate_model(logisticRegression_model_PortScan, X_test, y_test, X_scaled, y, \"logisticRegression\")\n",
    "evaluate_model(mlp_model_PortScan, X_test, y_test, X_scaled, y, \"mlp\")\n",
    "evaluate_model(lgb_model_PortScan, X_test, y_test, X_scaled, y, \"lgb\")\n",
    "evaluate_model(xgBoost_model_PortScan, X_test, y_test_encoded_series, X_scaled, y_encoded, \"xgBoost\")\n",
    "evaluate_model(svm_model_linearPortScan, X_test, y_test, X_scaled, y, \"svmLineal\")\n",
    "evaluate_model(svm_model_poly_degree_2PortScan, X_test, y_test, X_scaled, y, \"svmPoly2\")\n",
    "evaluate_model(svm_model_poly_degree_3PortScan, X_test, y_test, X_scaled, y, \"svmPoly3\")\n",
    "evaluate_model(svm_model_rbfPortScan, X_test, y_test, X_scaled, y, \"svmRbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javier\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n",
      "C:\\Users\\Javier\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:155: UserWarning: You have provided over 5k background samples! For better performance consider using smaller random sample.\n",
      "  warnings.warn(\"You have provided over 5k background samples! For better performance consider using smaller random sample.\")\n",
      "C:\\Users\\Javier\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Utilizar SHAP DeepExplainer\u001b[39;00m\n\u001b[0;32m     17\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model, X_train)\n\u001b[1;32m---> 18\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shap\\explainers\\_deep\\__init__.py:135\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:310\u001b[0m, in \u001b[0;36mTFDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[0;32m    309\u001b[0m feature_ind \u001b[38;5;241m=\u001b[39m model_output_ranks[j,i]\n\u001b[1;32m--> 310\u001b[0m sample_phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi_symbolic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_ind\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:366\u001b[0m, in \u001b[0;36mTFDeep.run\u001b[1;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[0;32m    363\u001b[0m         tf_execute\u001b[38;5;241m.\u001b[39mrecord_gradient \u001b[38;5;241m=\u001b[39m tf_backprop\u001b[38;5;241m.\u001b[39mrecord_gradient\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_out\n\u001b[1;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_with_overridden_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43manon\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:401\u001b[0m, in \u001b[0;36mTFDeep.execute_with_overridden_gradients\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# reinstate the backpropagatable check\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tf_gradients_impl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_IsBackpropagatable\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:359\u001b[0m, in \u001b[0;36mTFDeep.run.<locals>.anon\u001b[1;34m()\u001b[0m\n\u001b[0;32m    357\u001b[0m     v \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs[i]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    358\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[1;32m--> 359\u001b[0m final_out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m     tf_execute\u001b[38;5;241m.\u001b[39mrecord_gradient \u001b[38;5;241m=\u001b[39m tf_backprop\u001b[38;5;241m.\u001b[39m_record_gradient\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Construir el modelo MLP usando Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train_encoded, epochs=300, batch_size=10, verbose=0)\n",
    "\n",
    "# Utilizar SHAP DeepExplainer\n",
    "explainer = shap.DeepExplainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPLICABILIDAD GLOBAL\n",
    "#IMPORTANCIA DE CARACTERÍSTICAS CON COEFICIENTES DEL MODELO\n",
    "# Obtener los coeficientes del modelo SVM lineal\n",
    "\n",
    "# Function to display global feature importance\n",
    "def explicabilidad_global(model, feature_names):\n",
    "    try:\n",
    "        coeficientes = model.coef_[0]\n",
    "        importancias = np.abs(coeficientes)\n",
    "    except AttributeError:\n",
    "        print(\"Este modelo no proporciona coeficientes directamente.\")\n",
    "        return\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    importancias_normalizadas = scaler.fit_transform(importancias.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    plt.bar(range(len(importancias_normalizadas)), importancias_normalizadas)\n",
    "    plt.xticks(range(len(importancias_normalizadas)), feature_names, rotation='vertical')\n",
    "    plt.xlabel('Características')\n",
    "    plt.ylabel('Importancia')\n",
    "    plt.title('Importancia de características del modelo')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPENDENCIA DE CARACTERÍSTICAS ---------------------------------------------------------- rendimiento?\n",
    "# Function for feature dependence analysis\n",
    "def dependencia_caracteristicas(model, feature_idx):\n",
    "    ## DEPENDENCIA DE CARACTERÍSTICAS ---------------------------------------------------------- rendimiento?\n",
    "    # Encontrar el índice de la característica 'Destination Port' en X\n",
    "    indices = joblib.load('indices.pkl')\n",
    "    feature_name = top_20_feature_names[feature_idx]\n",
    "    # Dividir el conjunto de datos en entrenamiento y prueba\n",
    "    feature_index = X.columns.get_loc(feature_name)\n",
    "\n",
    "    # Encontrar la posición del feature_index dentro de la lista de índices\n",
    "    feature_position = np.where(indices == feature_index)[0][0]\n",
    "\n",
    "    # Crear el gráfico de dependencia parcial\n",
    "    disp = PartialDependenceDisplay.from_estimator(model, X_train_df, features=[feature_position])\n",
    "\n",
    "    plt.title('Partial Dependence Plot for ' f'{feature_name}')\n",
    "    #disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VALORES DE SHAP PARA CADA CARACTERÍSTICA DE MANERA GLOBAL\n",
    "# Function for SHAP global values\n",
    "def shap_valores_globales(model,modelName, feature_names, clase = 0):\n",
    "    shap.initjs()\n",
    "    if modelName == 'lineal':\n",
    "        try: \n",
    "            shap_values = joblib.load('shap_values_svmLineal.pkl')\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_svmLineal = shap.LinearExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_svmLineal,'explainer_shap_svmLineal.pkl')\n",
    "            shap_values = explainer_shap_svmLineal.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_svmLineal.pkl')\n",
    "        shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "    elif modelName == 'randomForest':\n",
    "        try: \n",
    "            shap_values=joblib.load('shap_values_randomForest.pkl')\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_randomForest = shap.TreeExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_randomForest,'explainer_shap_randomForest.pkl')\n",
    "            shap_values = explainer_shap_randomForest.shap_values(X_test_df)\n",
    "        shap_values_class0 = shap_values[:,:,clase]\n",
    "        shap.summary_plot(shap_values_class0, X_test, feature_names = feature_names)\n",
    "    elif modelName == 'lgb':\n",
    "        try:\n",
    "            shap_values = joblib.load('shap_values_lgb.pkl')\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_lgb = shap.TreeExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_lgb,'explainer_shap_lgb.pkl')\n",
    "            shap_values = explainer_shap_lgb.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_lgb.pkl')\n",
    "        shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "    elif modelName == 'xgBoost':\n",
    "        try: \n",
    "            shap_values = joblib.load('shap_values_xgBoost.pkl')\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_xgBoost = shap.TreeExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_xgBoost,'explainer_shap_xgBoost.pkl')\n",
    "            shap_values = explainer_shap_xgBoost.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_xgBoost.pkl')\n",
    "        shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "    elif modelName == 'logistic':\n",
    "        try: \n",
    "            shap_values = joblib.load('shap_values_logisticRegression.pkl')\n",
    "        except FileNotFoundError:\n",
    "            explainer_shap_logisticRegression = shap.LinearExplainer(model,X_train_df)\n",
    "            joblib.dump(explainer_shap_logisticRegression,'explainer_shap_logisticRegression.pkl')\n",
    "            shap_values = explainer_shap_logisticRegression.shap_values(X_test_df)\n",
    "            joblib.dump(shap_values,'shap_values_logisticRegression.pkl')    \n",
    "        shap.summary_plot(shap_values, X_test, feature_names = feature_names)\n",
    "    \n",
    "    '''elif modelName == 'mlp':\n",
    "        explainer_shap_mlp = shap.KernelExplainer(model.predict_proba ,X_train_df)\n",
    "        joblib.dump(explainer_shap_mlp,'explainer_shap_mlp.pkl')\n",
    "        shap_values = explainer_shap_mlp.shap_values(X_test_df)\n",
    "        joblib.dump(shap_values,'shap_values_mlp.pkl')\n",
    "        \n",
    "        elif modelName == 'poly2':\n",
    "        explainer_shap_svmPoly2 = shap.KernelExplainer(model.predict_proba, X_train_df)\n",
    "        #joblib.dump(explainer_shap_svmPoly2,'explainer_shap_poly2.pkl')\n",
    "        # Calcular los valores SHAP para todas las muestras de datos\n",
    "        shap_values = explainer_shap_svmPoly2.shap_values(X_test_df)\n",
    "        joblib.dump(shap_values,'shap_values_poly2.pkl')\n",
    "        # Mostrar un resumen de los valores SHAP\n",
    "        shap.summary_plot(shap_values, X_test_df, plot_type=\"bar\", feature_names=top_20_feature_names)'''\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPLICACION LOCAL PARA INSTANCIAS CON SHAP\n",
    "# Obtener una explicación interactiva para una muestra específica\n",
    "# Function for SHAP local explanation\n",
    "def shap_explicacion_local(explainerModelName, X_test, sample_index, feature_names):\n",
    "    explainerModel = joblib.load(f'explainer_shap_{explainerModelName}.pkl')\n",
    "    shap_values = joblib.load(f'shap_values_{explainerModelName}.pkl')\n",
    "    shap.initjs()\n",
    "    if explainerModelName == 'randomForest':\n",
    "        shap.force_plot(explainerModel.expected_value[0], shap_values[sample_index,:,1], X_test[sample_index,:], feature_names=feature_names,matplotlib=True,figsize=(50, 2.5))\n",
    "    elif explainerModelName == 'svmLineal' or explainerModelName == 'lgb' or explainerModelName == 'xgBoost' or explainerModelName == 'logisticRegression':\n",
    "        expl = explainerModel.shap_values(X_test[sample_index])\n",
    "        shap.force_plot(explainerModel.expected_value, expl, X_test[sample_index], feature_names = top_20_feature_names,matplotlib=True, figsize=(50, 2.5))\n",
    "    else: \n",
    "        print(\"No se ha introducido un modelo válido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RELACION DE VALORES SHAP CON VALORES REALES\n",
    "# Function for SHAP values vs real values\n",
    "def shap_valores_vs_reales(explainerModelName, X_test, feature_names, feature_index, clase=0):\n",
    "   if explainerModelName == 'randomForest':\n",
    "       # Choose an interaction index that is different from the feature index \n",
    "        shap_values = joblib.load(f'shap_values_{explainerModelName}.pkl')\n",
    "        shap.dependence_plot(feature_index, shap_values[:,:,0], X_test, feature_names=feature_names,interaction_index=feature_names[feature_index])\n",
    "   elif explainerModelName == 'svmLineal' or explainerModelName =='lgb' or explainerModelName == 'xgBoost' or explainerModelName == 'logisticRegression':\n",
    "       # Choose an interaction index that is different from the feature index \n",
    "        shap_values = joblib.load(f'shap_values_{explainerModelName}.pkl')\n",
    "        shap.dependence_plot(feature_index, shap_values, X_test, feature_names=feature_names,interaction_index=feature_names[feature_index])\n",
    "   else: \n",
    "        print(\"No se ha introducido un modelo válido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE IMPORTANCIA DE CARACTERÍSTICAS PARA INSTANCIAS CONCRETAS CON LIME (GRAFICO DISPERSION)\n",
    "# Obtener las características y sus pesos\n",
    "def lime_analisis_caracteristicasDispersion(model,modelName, X_train, X_test, sample_index, feature_names):\n",
    "    if modelName != 'randomForest' and modelName != 'lgb' and modelName != 'xgBoost' and modelName != 'mlp' and modelName != 'logisticRegression':\n",
    "        try: \n",
    "            calibrated = joblib.load('calibrated_'f'{modelName}')\n",
    "        except FileNotFoundError:\n",
    "            calibrated = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "            calibrated.fit(X_train, y_train)\n",
    "            joblib.dump(calibrated, 'calibrated_'f'{modelName}')\n",
    "    elif modelName == 'randomForest':\n",
    "        calibrated = randomForest_modelPortScan\n",
    "    elif modelName == 'lgb':\n",
    "        calibrated = lgb_model_PortScan\n",
    "    elif modelName == 'xgBoost':\n",
    "        calibrated = xgBoost_model_PortScan\n",
    "    elif modelName == 'logisticRegression':\n",
    "        calibrated = logisticRegression_model_PortScan\n",
    "    elif modelName == 'mlp':\n",
    "        calibrated = mlp_model_PortScan\n",
    "    explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['BENIGN', 'DDoS'], discretize_continuous=True)\n",
    "    expLime = explainer_lime.explain_instance(X_test[sample_index], calibrated.predict_proba)\n",
    "    weights = expLime.as_list()\n",
    "    feature_importance = {feat: weight for feat, weight in weights}\n",
    "    # Obtener las características y sus pesos\n",
    "    features = list(feature_importance.keys())\n",
    "    weights = list(feature_importance.values())\n",
    "\n",
    "    # Crear un gráfico de dispersión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(weights, features, color='green', alpha=0.5)\n",
    "    plt.xlabel('Peso')\n",
    "    plt.ylabel('Característica')\n",
    "    plt.title('Importancia de las características según LIME')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE IMPORTANCIA DE CARACTERÍSTICAS PARA INSTANCIAS CONCRETAS CON LIME \n",
    "# Obtener las características y sus pesos\n",
    "def lime_analisis_caracteristicas(model,modelName, X_train, X_test, sample_index, feature_names):\n",
    "    if modelName != 'randomForest' and modelName != 'lgb' and modelName != 'xgBoost' and modelName != 'mlp' and modelName != 'logisticRegression':\n",
    "        try: \n",
    "            calibrated = joblib.load('calibrated_'f'{modelName}')\n",
    "        except FileNotFoundError:\n",
    "            calibrated = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "            calibrated.fit(X_train, y_train)\n",
    "            joblib.dump(calibrated, 'calibrated_'f'{modelName}')\n",
    "    elif modelName == 'randomForest':\n",
    "        calibrated = randomForest_modelPortScan\n",
    "    elif modelName == 'lgb':\n",
    "        calibrated = lgb_model_PortScan\n",
    "    elif modelName == 'xgBoost':\n",
    "        calibrated = xgBoost_model_PortScan\n",
    "    elif modelName == 'logisticRegression':\n",
    "        calibrated = logisticRegression_model_PortScan\n",
    "    elif modelName == 'mlp':\n",
    "        calibrated = mlp_model_PortScan\n",
    "    explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['BENIGN', 'DDoS'], discretize_continuous=True)\n",
    "    expLime = explainer_lime.explain_instance(X_test[sample_index], calibrated.predict_proba)\n",
    "    weights = expLime.as_list()\n",
    "    feature_importance = {feat: weight for feat, weight in weights}\n",
    "    # Obtener las características y sus pesos\n",
    "    features = list(feature_importance.keys())\n",
    "    weights = list(feature_importance.values())\n",
    "\n",
    "    # Crear un gráfico de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(features, weights, color='skyblue')\n",
    "    plt.xlabel('Peso')\n",
    "    plt.ylabel('Característica')\n",
    "    plt.title('Importancia de las características según LIME')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPLICACION LOCAL PARA INSTANCIAS CON LIME\n",
    "# Function for LIME analysis for specific instances\n",
    "def lime_analisis_instancias(model,modelName, X_train, X_test, sample_index, feature_names):\n",
    "    if modelName != 'randomForest' and modelName != 'lgb' and modelName != 'xgBoost' and modelName != 'mlp' and modelName != 'logisticRegression':\n",
    "        try: \n",
    "            calibrated = joblib.load('calibrated_'f'{modelName}')\n",
    "        except FileNotFoundError:\n",
    "            calibrated = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "            calibrated.fit(X_train, y_train)\n",
    "            joblib.dump(calibrated, 'calibrated_'f'{modelName}')\n",
    "    elif modelName == 'randomForest':\n",
    "        calibrated = randomForest_modelPortScan\n",
    "    elif modelName == 'lgb':\n",
    "        calibrated = lgb_model_PortScan\n",
    "    elif modelName == 'xgBoost':\n",
    "        calibrated = xgBoost_model_PortScan\n",
    "    elif modelName == 'logisticRegression':\n",
    "        calibrated = logisticRegression_model_PortScan\n",
    "    elif modelName == 'mlp':\n",
    "        calibrated = mlp_model_PortScan\n",
    "\n",
    "    explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['BENIGN', 'DDoS'], discretize_continuous=True)\n",
    "    exp = explainer_lime.explain_instance(X_test[sample_index], calibrated.predict_proba)\n",
    "    exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE RESIDUOS\n",
    "def analisis_residuos(modelName,X_test,y_test):\n",
    "    # Copiar y_test para no modificar los datos originales\n",
    "\n",
    "    y_test_encoded = y_test.copy()\n",
    "\n",
    "    # Inicializar el codificador de etiquetas\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Ajustar el codificador de etiquetas y transformar las etiquetas\n",
    "    y_test_encoded = label_encoder.fit_transform(y_test_encoded)\n",
    "\n",
    "\n",
    "    if modelName != 'randomForest' and modelName != 'lgb'and modelName != 'xgBoost' and modelName != 'mlp' and modelName != 'logistic':\n",
    "        calibrated = joblib.load('calibrated_'f'{modelName}')\n",
    "    elif modelName == 'randomForest':\n",
    "        calibrated = randomForest_modelPortScan\n",
    "    elif modelName == 'lgb':\n",
    "        calibrated = lgb_model_PortScan\n",
    "    elif modelName == 'xgBoost':\n",
    "        calibrated = xgBoost_model_PortScan\n",
    "    elif modelName == 'mlp':\n",
    "        calibrated = mlp_model_PortScan\n",
    "    elif modelName == 'logistic':\n",
    "        calibrated = logisticRegression_model_PortScan\n",
    "        \n",
    "    # Realizar predicciones de probabilidad en lugar de clases\n",
    "    y_pred_prob = calibrated.predict_proba(X_test)[:, 1]  # Probabilidad de pertenencia a la clase positiva\n",
    "\n",
    "    # Calcular los residuos\n",
    "    residuos = y_test_encoded - y_pred_prob\n",
    "\n",
    "    # Visualizar los residuos\n",
    "    plt.scatter(y_pred_prob, residuos)\n",
    "    plt.xlabel('Probabilidad Predicha (Clase Positiva)')\n",
    "    plt.ylabel('Residuos')\n",
    "    plt.title('Análisis de Residuos para Modelo ' f'{modelName}')\n",
    "    plt.axhline(y=0, color='r', linestyle='-')  # Línea horizontal en y=0 para referencia\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Crear un histograma de los residuos\n",
    "    plt.hist(residuos, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Residuos')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title('Histograma de Residuos para Modelo ' f'{modelName}')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')  # Línea vertical en x=0 para referencia\n",
    "    plt.show()\n",
    "\n",
    "    # Crear un gráfico de densidad de kernel de los residuos\n",
    "    sns.kdeplot(residuos, shade=True)\n",
    "    plt.xlabel('Residuos')\n",
    "    plt.ylabel('Densidad de Probabilidad')\n",
    "    plt.title('Densidad de Kernel de Residuos para Modelo ' f'{modelName}')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')  # Línea vertical en x=0 para referencia\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE SENSIBILIDAD\n",
    "# La gráfica que has generado muestra cómo varía la predicción media del modelo SVM lineal \n",
    "# a medida que se ajusta el valor de la primera característica. En este caso, la gráfica indica que a \n",
    "# medida que el valor de la primera característica aumenta, \n",
    "# la probabilidad predicha de la clase positiva ('DDoS') disminuye.\n",
    "# Asegurarse de que las predicciones sean numéricas\n",
    "# Rango de valores para la primera característica\n",
    "\n",
    "def analisis_sensibilidad(model, X_test, feature_idx):\n",
    "\n",
    "    valores_caracteristica = np.linspace(X_test[:, feature_idx].min(), X_test[:, feature_idx].max(), 100)\n",
    "    X_test_mean = X_test.mean(axis=0)\n",
    "\n",
    "    # Inicializar una lista para almacenar las predicciones\n",
    "    predicciones = []\n",
    "\n",
    "    # Realizar predicciones para cada valor de la característica\n",
    "    for valor in valores_caracteristica:\n",
    "        X_test_modificado = X_test.copy()\n",
    "        X_test_modificado[:, 0] = valor\n",
    "        \n",
    "        # Realizar las predicciones y asegurarse de que sean numéricas\n",
    "        predicciones_modelo = model.predict(X_test_modificado)\n",
    "        \n",
    "        # Calcular la media de las predicciones (asegurándose de que sean valores numéricos)\n",
    "        prediccion_media = np.mean([1 if pred == 'PortScan' or pred == 1 else 0 for pred in predicciones_modelo])\n",
    "        predicciones.append(prediccion_media)\n",
    "\n",
    "    # Visualizar el cambio en las predicciones\n",
    "    plt.plot(valores_caracteristica, predicciones)\n",
    "    feature_name = top_20_feature_names[feature_idx]\n",
    "    plt.xlabel('Valor de ' f'{feature_name}')\n",
    "    plt.ylabel('Predicción Media')\n",
    "    plt.title('Análisis de Sensibilidad')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANALISIS DE INFLUENCIA\n",
    "\n",
    "def analisis_influencia (model, modelName, X_test, y_test, range_exclusion):\n",
    "    # Realizar predicciones con el conjunto de pruebas original\n",
    "    if modelName == 'xgBoost':\n",
    "        pos_label = 1\n",
    "    else:\n",
    "        pos_label = 'PortScan'\n",
    "    \n",
    "    predicciones_original = model.predict(X_test)\n",
    "\n",
    "    # Calcular las métricas de rendimiento\n",
    "    accuracy_original = accuracy_score(y_test, predicciones_original)\n",
    "    precision_original = precision_score(y_test, predicciones_original, pos_label=pos_label)\n",
    "    recall_original = recall_score(y_test, predicciones_original, pos_label=pos_label)\n",
    "    f1_original = f1_score(y_test, predicciones_original, pos_label=pos_label)\n",
    "\n",
    "    # Excluir las primeras 20000 instancias del conjunto de pruebas\n",
    "    X_test_excluido = np.delete(X_test, range_exclusion, axis=0)\n",
    "    y_test_excluido = np.delete(y_test, range_exclusion)\n",
    "\n",
    "    # Realizar predicciones con el conjunto de pruebas excluyendo las primeras 20000 instancias\n",
    "    predicciones_excluido = model.predict(X_test_excluido)\n",
    "\n",
    "    # Calcular las métricas de rendimiento nuevamente\n",
    "    accuracy_excluido = accuracy_score(y_test_excluido, predicciones_excluido)\n",
    "    precision_excluido = precision_score(y_test_excluido, predicciones_excluido, pos_label=pos_label)\n",
    "    recall_excluido = recall_score(y_test_excluido, predicciones_excluido, pos_label=pos_label)\n",
    "    f1_excluido = f1_score(y_test_excluido, predicciones_excluido, pos_label=pos_label)\n",
    "\n",
    "    # Comparar las métricas antes y después de la exclusión\n",
    "    metricas = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    valores_original = [accuracy_original, precision_original, recall_original, f1_original]\n",
    "    valores_excluido = [accuracy_excluido, precision_excluido, recall_excluido, f1_excluido]\n",
    "\n",
    "    # Visualización de la comparación de métricas\n",
    "    x = np.arange(len(metricas))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, valores_original, width, label='Original')\n",
    "    rects2 = ax.bar(x + width/2, valores_excluido, width, label='Excluido')\n",
    "\n",
    "    # Añadir etiquetas, título y leyenda\n",
    "    ax.set_xlabel('Métricas')\n",
    "    ax.set_ylabel('Valores')\n",
    "    ax.set_title('Comparación de Métricas de Rendimiento antes y después de la Exclusión')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metricas)\n",
    "    ax.legend()\n",
    "\n",
    "    # Añadir valores encima de las barras\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('%.2f' % height,\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 puntos de offset vertical\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PERMUTACION DE CARACTERÍSTICAS\n",
    "def permutacion_de_caracteristicas(model, X_test, y_test, feature):\n",
    "    precision_original = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"Precisión original:\", precision_original)\n",
    "\n",
    "    # Función para calcular la precisión después de permutar una característica específica\n",
    "    def precision_despues_permutacion(X, y, modelo, caracteristica_permutada):\n",
    "        X_permutado = X.copy()\n",
    "        np.random.shuffle(X_permutado[:, caracteristica_permutada])  # Permutar los valores de la característica\n",
    "        return accuracy_score(y, modelo.predict(X_permutado))\n",
    "\n",
    "    # Calcular la precisión después de permutar la característica seleccionada\n",
    "    precision_permutacion = precision_despues_permutacion(X_test, y_test, model, feature)\n",
    "    print(\"Precisión después de la permutación de la característica:\", precision_permutacion)\n",
    "\n",
    "    # Calcular el cambio en la precisión debido a la permutación de características\n",
    "    cambio_precision = precision_permutacion - precision_original\n",
    "    print(\"Cambio en la precisión:\", cambio_precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicabilidad_global(logisticRegression_model_DDOS, top_20_feature_names) # Bien con svm lineal y regresion logistica\n",
    "#dependencia_caracteristicas(mlp_model_DDOS, feature_idx=17) ## Fracaso tema rendimiento con cualquier modelo (lgb bien y xgBoost, analizar).\n",
    "#analisis_residuos('logistic', X_test, y_test) # Bien\n",
    "#analisis_sensibilidad(mlp_model_DDOS, X_test, 17) ## rendimiento mal (con lgb bien, xgBoost, mlp, logistic)\n",
    "#analisis_influencia(logisticRegression_model_DDOS,'mlp', X_test_df, y_test, slice(0, 20000)) ## Bien (cambia el y_test segun modelo encoded para xgBoost)\n",
    "#permutacion_de_caracteristicas(mlp_model_DDOS, X_test, y_test, 17) # Bien\n",
    "#shap_valores_globales(logisticRegression_model_DDOS, 'logistic', top_20_feature_names) ## Fracaso con svm poly y rbf, mlp por rendimiento del shap.KernelExplainer\n",
    "#shap_explicacion_local('logisticRegression', X_test, 2, top_20_feature_names) ## Fracaso con svm poly y rbf por rendimiento del shap.KernelExplainer\n",
    "#shap_valores_vs_reales('logisticRegression', X_test_df, top_20_feature_names, 17) ## Fracaso con svm poly y rbf por rendimiento del shap.KernelExplainer\n",
    "#lime_analisis_instancias('logisticRegression', X_train, X_test, sample_index=2, feature_names=top_20_feature_names) # Bien\n",
    "#lime_analisis_caracteristicas('mlp', X_train, X_test, sample_index=2, feature_names=top_20_feature_names) # Bien\n",
    "#lime_analisis_caracteristicasDispersion('logisticRegression', X_train, X_test, sample_index=2, feature_names=top_20_feature_names) # Bien\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
